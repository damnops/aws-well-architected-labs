{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"README.html","text":"AWS Well-Architected Labs Introduction The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Labs: The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Home"},{"location":"README.html#aws-well-architected-labs","text":"","title":"AWS Well-Architected Labs"},{"location":"README.html#introduction","text":"The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.","title":"Introduction"},{"location":"README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"README.html#labs","text":"The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool","title":"Labs:"},{"location":"README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/README.html","text":"AWS Well-Architected Cost Optimization Labs http://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Labs: Cost Fundamentals These labs must be done in order from 1 to 5, starting at the 100 level labs. These labs are designed to start you on your Cost Optimization journey. They cover the required steps to enable your account and the features within it for Cost Optimization. They also focus on addressing the core areas of the Well-Architected Cost Optimization pillar, with simple but effective exercises. 100 Level 100 #1 AWS Account Setup 100 #2 Cost and Usage Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization 200 Level 200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization Cost and Usage Analysis These labs focus on analysis of your cost and usage. The 100 and 200 level are from the fundamentals series and should be done before any other labs. The 300 cover specific areas of analysis and can be done independently at any time. 100 and 200 Level 100 #4 Cost and Usage Analysis 200 #4 Cost and Usage Analysis 300 300 Billing Analysis - Automated CUR Updates and Ingestion 300 Billing Analysis - Multi Account CUR Access 300 Billing Analysis - Splitting and Sharing CUR Access","title":"Overview"},{"location":"Cost/README.html#aws-well-architected-cost-optimization-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Introduction"},{"location":"Cost/README.html#labs","text":"","title":"Labs:"},{"location":"Cost/README.html#cost-fundamentals","text":"These labs must be done in order from 1 to 5, starting at the 100 level labs. These labs are designed to start you on your Cost Optimization journey. They cover the required steps to enable your account and the features within it for Cost Optimization. They also focus on addressing the core areas of the Well-Architected Cost Optimization pillar, with simple but effective exercises.","title":"Cost Fundamentals"},{"location":"Cost/README.html#100-level","text":"100 #1 AWS Account Setup 100 #2 Cost and Usage Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization","title":"100 Level"},{"location":"Cost/README.html#200-level","text":"200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization","title":"200 Level"},{"location":"Cost/README.html#cost-and-usage-analysis","text":"These labs focus on analysis of your cost and usage. The 100 and 200 level are from the fundamentals series and should be done before any other labs. The 300 cover specific areas of analysis and can be done independently at any time.","title":"Cost and Usage Analysis"},{"location":"Cost/README.html#100-and-200-level","text":"100 #4 Cost and Usage Analysis 200 #4 Cost and Usage Analysis","title":"100 and 200 Level"},{"location":"Cost/README.html#300","text":"300 Billing Analysis - Automated CUR Updates and Ingestion 300 Billing Analysis - Multi Account CUR Access 300 Billing Analysis - Splitting and Sharing CUR Access","title":"300"},{"location":"Cost/thankyou.html","text":"Thankyou for your feedback Return to the Cost Labs","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#thankyou-for-your-feedback","text":"","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#return-to-the-cost-labs","text":"","title":"Return to the Cost Labs"},{"location":"Cost/Cost_Fundamentals/README.html","text":"Cost Optimization - Fundamentals http://wellarchitectedlabs.com Introduction These labs must be done in order from 1 to 5, starting at the 100 level labs. These labs are designed to start you on your Cost Optimization journey. They cover the required steps to enable your account and the features within it for Cost Optimization. They also focus on addressing the core areas of the Well-Architected Cost Optimization pillar, with simple but effective exercises. Cost Fundamentals 100 Level 100 #1 AWS Account Setup 100 #2 Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization 200 Level 200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization","title":"Fundamentals Overview"},{"location":"Cost/Cost_Fundamentals/README.html#cost-optimization-fundamentals","text":"http://wellarchitectedlabs.com","title":"Cost Optimization - Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#introduction","text":"These labs must be done in order from 1 to 5, starting at the 100 level labs. These labs are designed to start you on your Cost Optimization journey. They cover the required steps to enable your account and the features within it for Cost Optimization. They also focus on addressing the core areas of the Well-Architected Cost Optimization pillar, with simple but effective exercises.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/README.html#cost-fundamentals","text":"","title":"Cost Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#100-level","text":"100 #1 AWS Account Setup 100 #2 Governance 100 #4 Cost and Usage Analysis 100 #5 Cost Visualization","title":"100 Level"},{"location":"Cost/Cost_Fundamentals/README.html#200-level","text":"200 #2 Governance 200 #3 Pricing Models 200 #4 Cost and Usage Analysis 200 #5 Cost Visualization","title":"200 Level"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html","text":"Level 100: AWS Account Setup http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework. Goals Implement an account structure Configure billing services Prerequisites Multiple AWS accounts (at least two) Root user access to the master account Permissions required Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#level-100-aws-account-setup","text":"http://wellarchitectedlabs.com","title":"Level 100: AWS Account Setup"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#introduction","text":"This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#goals","text":"Implement an account structure Configure billing services","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#prerequisites","text":"Multiple AWS accounts (at least two) Root user access to the master account","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#permissions-required","text":"Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#best-practice-checklist","text":"[ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html","text":"Level 100: AWS Account Setup: Lab Guide Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Tear down Rate this Lab Feedback survey 1. Configure IAM access to your billing NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing. 2. Create an account structure NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multilpe AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data. 2.1 Create an AWS Organization You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to. 2.2 Join member accounts You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization. 3. Configure billing account settings It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update : 4. Configure Cost and Usage Reports Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient. 4.1 Configure a Cost and Usage Report If you configure multilpe Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multilpe CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered. 4.2 Enable monthly billing report The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences : 5. Enable AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated: 6. Enable AWS-Generated Cost Allocation Tags Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated: 7. Tear down This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used. 8. Rate this lab 9. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#level-100-aws-account-setup-lab-guide","text":"","title":"Level 100: AWS Account Setup: Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#table-of-contents","text":"Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Tear down Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#1-configure-iam-access-to-your-billing","text":"NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing.","title":"1. Configure IAM access to your billing"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#2-create-an-account-structure","text":"NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multilpe AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data.","title":"2. Create an account structure"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#21-create-an-aws-organization","text":"You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to.","title":"2.1 Create an AWS Organization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#22-join-member-accounts","text":"You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization.","title":"2.2 Join member accounts"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#3-configure-billing-account-settings","text":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update :","title":"3. Configure billing account settings"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#4-configure-cost-and-usage-reports","text":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient.","title":"4. Configure Cost and Usage Reports"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#41-configure-a-cost-and-usage-report","text":"If you configure multilpe Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multilpe CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.","title":"4.1 Configure a Cost and Usage Report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#42-enable-monthly-billing-report","text":"The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences :","title":"4.2 Enable monthly billing report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#5-enable-aws-cost-explorer","text":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated:","title":"5. Enable AWS Cost Explorer"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#6-enable-aws-generated-cost-allocation-tags","text":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated:","title":"6. Enable AWS-Generated Cost Allocation Tags"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#7-tear-down","text":"This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used.","title":"7. Tear down"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#9-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"9. Survey "},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/master_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ organizations:InviteAccountToOrganization , organizations:ListRoots , aws-portal:ModifyAccount , s3:ListBucketVersions , organizations:DescribeAccount , s3:CreateBucket , s3:ListBucket , s3:GetBucketPolicy , organizations:ListChildren , aws-portal:ModifyBilling , organizations:ListCreateAccountStatus , organizations:DescribeOrganization , organizations:EnableAllFeatures , aws-portal:ViewBilling , organizations:DescribeHandshake , s3:PutBucketVersioning , organizations:DescribeCreateAccountStatus , organizations:CreateOrganization , s3:GetBucketPolicyStatus , s3:GetBucketPublicAccessBlock , s3:PutBucketPublicAccessBlock , aws-portal:ViewAccount , s3:GetBucketVersioning , organizations:ListAWSServiceAccessForOrganization , s3:DeleteBucketPolicy , organizations:ListHandshakesForOrganization , organizations:ListAccounts , iam:CreateServiceLinkedRole , s3:ListAllMyBuckets , s3:PutBucketPolicy , s3:GetBucketLocation ], Resource : * } ] }","title":"Master policy"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/member_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:CreateServiceLinkedRole , organizations:AcceptHandshake , organizations:DescribeHandshake ], Resource : [ arn:aws:iam::*:role/* , arn:aws:organizations::*:handshake/o-*/*/h-* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : organizations:DescribeAccount , Resource : arn:aws:organizations::*:account/o-*/* }, { Sid : VisualEditor2 , Effect : Allow , Action : [ organizations:ListHandshakesForAccount , organizations:DescribeOrganization , organizations:DescribeCreateAccountStatus ], Resource : * } ] }","title":"Member policy"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html","text":"Level 100: Cost and Usage Governance http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend Prerequisites An AWS Account AWS Account Setup has been completed Permissions required ./Code/IAM_policy IAM policy required for this lab Start the Lab! Best Practice Checklist [ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#level-100-cost-and-usage-governance","text":"http://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#goals","text":"Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account AWS Account Setup has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 100: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create a cost optimization team Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - RI Coverage Tear down Rate this Lab Feedback survey 1. Create a cost optimization team We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab 1.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log in and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 1.2 Create an IAM Group This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 1.3 Create an IAM User For this lab we will create a user and join them to the group above. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 2. Create and implement an AWS Budget for monthly forecasted usage Budgets allow you to manage cost and usage by providing notifications when usage or cost are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account. Create a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current forecast will exceed the budget (it is red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount. 3. Create and implement an AWS Budget for EC2 actual cost We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for other services than EC2. 4. Create and implement an AWS Budget for EC2 Instance RI coverage We will create a monthly RI coverage budget which will notify if the coverage of Reserved Instances for EC2 is below the specified amount. Click Create budget : Select Reservation budget , and click Set your budget : For Reservation budget type Select RI Coverage , enter a Name , select EC2-Instances as the Service , enter a Coverage threshold of 80% and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an RI Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes: 5. Tear down NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations. Delete a budget We will delete all three budgets that were configured in sections 2,3 and 4. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_RI_Coverage : Click on the 3 dot menu in the top right, select Delete : ALl budgets should be deleted that were created in this workshop: 6. Rate this lab 7. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#level-100-cost-and-usage-governance","text":"","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Create a cost optimization team Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - RI Coverage Tear down Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#1-create-a-cost-optimization-team","text":"We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab","title":"1. Create a cost optimization team "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#11-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log in and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"1.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#12-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"1.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#13-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"1.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-and-implement-an-aws-budget-for-monthly-forecasted-usage","text":"Budgets allow you to manage cost and usage by providing notifications when usage or cost are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account.","title":"2. Create and implement an AWS Budget for monthly forecasted usage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#create-a-monthly-cost-budget-for-your-account","text":"We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current forecast will exceed the budget (it is red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount.","title":"Create a monthly cost budget for your account"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-and-implement-an-aws-budget-for-ec2-actual-cost","text":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : (enter a name), Budgeted amount : (enter an amount a lot LESS than last months cost), Budget effective dates : Select Recurring Budget and start month is the current month, Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for other services than EC2.","title":"3. Create and implement an AWS Budget for EC2 actual cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#4-create-and-implement-an-aws-budget-for-ec2-instance-ri-coverage","text":"We will create a monthly RI coverage budget which will notify if the coverage of Reserved Instances for EC2 is below the specified amount. Click Create budget : Select Reservation budget , and click Set your budget : For Reservation budget type Select RI Coverage , enter a Name , select EC2-Instances as the Service , enter a Coverage threshold of 80% and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an RI Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes:","title":"4. Create and implement an AWS Budget for EC2 Instance RI coverage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#5-tear-down","text":"NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations.","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-budget","text":"We will delete all three budgets that were configured in sections 2,3 and 4. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_RI_Coverage : Click on the 3 dot menu in the top right, select Delete : ALl budgets should be deleted that were created in this workshop:","title":"Delete a budget"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#7-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"7. Survey "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Code/IAM_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:ListPolicies , iam:GetPolicyVersion , iam:CreateGroup , iam:GetPolicy , iam:DeletePolicy , iam:DetachGroupPolicy , iam:ListGroupPolicies , iam:AttachUserPolicy , iam:CreateUser , iam:GetGroup , iam:CreatePolicy , iam:CreateLoginProfile , iam:AddUserToGroup , iam:ListPolicyVersions , iam:AttachGroupPolicy , iam:ListUsers , iam:ListAttachedGroupPolicies , iam:ListGroups , iam:GetGroupPolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion , iam:GetLoginProfile ], Resource : * } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html","text":"Level 100: Cost and Usage Analysis http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#level-100-cost-and-usage-analysis","text":"http://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 100: Cost and Usage Analysis Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab Survey 1. View your AWS Invoices At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below: 2. View your cost and usage in detail You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage: 3. Download your monthly cost and usage file It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis . 4. Tear down There is no configuration performed within this lab, so no teardown is required. 5. Rate this lab 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-100-cost-and-usage-analysis","text":"","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab Survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-view-your-aws-invoices","text":"At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below:","title":"1. View your AWS Invoices "},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-view-your-cost-and-usage-in-detail","text":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage:","title":"2. View your cost and usage in detail"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-download-your-monthly-cost-and-usage-file","text":"It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis .","title":"3. Download your monthly cost and usage file"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"There is no configuration performed within this lab, so no teardown is required.","title":"4. Tear down"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html","text":"Level 100: Billing Visualization http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#level-100-billing-visualization","text":"http://wellarchitectedlabs.com","title":"Level 100: Billing Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html","text":"Level 100: Cost Visualization Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab Survey 1. View your cost and usage by service AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters. 2. View your cost and usage by account We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multilpe accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters. 3. View your Reserved Instance coverage To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage. 4. Create custom EC2 reports We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports: 5. Tear down We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available: 6. Rate this lab 7. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#level-100-cost-visualization","text":"","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab Survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#1-view-your-cost-and-usage-by-service","text":"AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters.","title":"1. View your cost and usage by service "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#2-view-your-cost-and-usage-by-account","text":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multilpe accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multilpe filters. You can continue to modify the report by timeframe and apply other filters.","title":"2. View your cost and usage by account "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#3-view-your-reserved-instance-coverage","text":"To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage.","title":"3. View your Reserved Instance coverage "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#4-create-custom-ec2-reports","text":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports:","title":"4. Create custom EC2 reports "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#5-tear-down","text":"We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available:","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#7-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"7. Survey "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html","text":"Level 200: Cost and Usage Governance http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage Prerequisites An AWS Account Completed all previous labs in the Cost Fundamentals series Permissions required ./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#level-200-cost-and-usage-governance","text":"http://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#goals","text":"Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 200: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create a cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab Feedback survey 1. Create a cost optimization team We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. 1.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log into the console as an IAM user with the required permissions and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 1.2 Create an IAM Group This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): We will now add more policies, click on Customer Managed and select AWS Managed : Type Athena into the filter box, select both policies, and click Next Step : Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 1.3 Create an IAM User For this lab we will create a user and join them to the group above. NOTE: it is best practice to have Multi Factor Authentication (MFA) enabled for all users. We omit this step here for brevity and simplicity, and should only be implemented as a demonstration before being removed/rectified. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 2. Create an IAM Policy to restrict service usage by region To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 2.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy. 2.2 Apply it to a group Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group. 2.3 Verify the policy is in effect Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. 3. Create an IAM Policy to restirct EC2 usage by family AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 3.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter a Name , a Description , and click on Create Policy : 3.2 Attach the policy to the group Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy : 3.3 Verify the policy is in effect Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above: 4. Extend an IAM Policy to restrict EC2 usage by instance size We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed. 4.1 Extend the EC2Family_Restrict IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes : 4.2 Verify the policy is in effect Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size. 5. Create an IAM policy to restrict EBS Volume creation by volume type Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 5.1 Create the IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Enter a Name and a Description , and click Create policy : 5.2 Attach the policy to the Cost Optimization group Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy : 5.3 Verify the policy is in effect Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above: 6. Tear down NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations. Delete a security group When you attempted to, and successfully launched instances above it created a launch-wizard security group automatically, which you will need to delete. Go to the EC2 Dashboard: Confirm the instances launched as part of this exercise are terminated. Click on Instances on the left and view the instances. You can use the Launch Time column to verify this. Select Security Groups under NETWORK AND SECURITY on the left: Click the checkbox next to the security group you need to delete: NOTE: you took note of the specific group in the exercise above, you can also use the Description column which will show when it was created. Click Actions , then select Delete Security Group : Click Yes, Delete : Remove a policy from a group We will remove the IAM policies from our cost optimization group. Go to the IAM Console: Select Groups from the left menu: Click on the CostOptimization group (that was created previously): Click on Permissions : Click on Detach Policy next to the EC2_FamilyRestrict and also EC2EBS_Restrict Policy Names: Click Detach : Repeat the steps above for EC2EBS_Restrict : Click Detach : Delete a policy We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies. 7. Rate this lab 8. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#level-200-cost-and-usage-governance","text":"","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Create a cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#1-create-a-cost-optimization-team","text":"We are going to create a cost optimization team. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms.","title":"1. Create a cost optimization team "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#11-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. Log into the console as an IAM user with the required permissions and go to the IAM Service page: Select Policies from the left menu: Select Create Policy : Select the JSON tab: Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"1.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#12-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. While in the IAM console, select Groups from the left menu: Click on Create New Group : Enter a Group Name and click Next Step : Click Policy Type and select Customer Managed : Select the CostOptimization_Summit policy (created previously): We will now add more policies, click on Customer Managed and select AWS Managed : Type Athena into the filter box, select both policies, and click Next Step : Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"1.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#13-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. NOTE: it is best practice to have Multi Factor Authentication (MFA) enabled for all users. We omit this step here for brevity and simplicity, and should only be implemented as a demonstration before being removed/rectified. In the IAM console, select Users from the left menu: Click Add user : Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : Select the CostOptimization group (created previously), and click Next: Tags : Click Next Review : Click Create user : Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"1.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-an-iam-policy-to-restrict-service-usage-by-region","text":"To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"2. Create an IAM Policy to restrict service usage by region "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#21-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy.","title":"2.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#22-apply-it-to-a-group","text":"Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group.","title":"2.2 Apply it to a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#23-verify-the-policy-is-in-effect","text":"Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region.","title":"2.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-an-iam-policy-to-restirct-ec2-usage-by-family","text":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"3. Create an IAM Policy to restirct EC2 usage by family "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#31-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter a Name , a Description , and click on Create Policy :","title":"3.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#32-attach-the-policy-to-the-group","text":"Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy :","title":"3.2 Attach the policy to the group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#33-verify-the-policy-is-in-effect","text":"Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above:","title":"3.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#4-extend-an-iam-policy-to-restrict-ec2-usage-by-instance-size","text":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.","title":"4. Extend an IAM Policy to restrict EC2 usage by instance size "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#41-extend-the-ec2family_restrict-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes :","title":"4.1 Extend the EC2Family_Restrict IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#42-verify-the-policy-is-in-effect","text":"Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size.","title":"4.2 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#5-create-an-iam-policy-to-restrict-ebs-volume-creation-by-volume-type","text":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"5. Create an IAM policy to restrict EBS Volume creation by volume type "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#51-create-the-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Enter a Name and a Description , and click Create policy :","title":"5.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#52-attach-the-policy-to-the-cost-optimization-group","text":"Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy :","title":"5.2 Attach the policy to the Cost Optimization group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#53-verify-the-policy-is-in-effect","text":"Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above:","title":"5.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#6-tear-down","text":"NOTE: The cost optimization user, group and policies are required for the completion of the fundamental labs. If you remove these resources you will not be able to complete the labs. There is no tear down for this component as it is best practices to have this group created in all organizations.","title":"6. Tear down "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-security-group","text":"When you attempted to, and successfully launched instances above it created a launch-wizard security group automatically, which you will need to delete. Go to the EC2 Dashboard: Confirm the instances launched as part of this exercise are terminated. Click on Instances on the left and view the instances. You can use the Launch Time column to verify this. Select Security Groups under NETWORK AND SECURITY on the left: Click the checkbox next to the security group you need to delete: NOTE: you took note of the specific group in the exercise above, you can also use the Description column which will show when it was created. Click Actions , then select Delete Security Group : Click Yes, Delete :","title":"Delete a security group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#remove-a-policy-from-a-group","text":"We will remove the IAM policies from our cost optimization group. Go to the IAM Console: Select Groups from the left menu: Click on the CostOptimization group (that was created previously): Click on Permissions : Click on Detach Policy next to the EC2_FamilyRestrict and also EC2EBS_Restrict Policy Names: Click Detach : Repeat the steps above for EC2EBS_Restrict : Click Detach :","title":"Remove a policy from a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-policy","text":"We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies.","title":"Delete a policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#8-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"8. Survey "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2EBS_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Deny , Action : ec2:* , Resource : * , Condition : { StringEquals : { ec2:VolumeType : io1 } } } ] }","title":"EC2EBS Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2Family_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : ec2:* , Resource : * , Condition : { ForAllValues:StringLike : { ec2:InstanceType : [ t3.* , a1.* , m5.* ] } } } ] }","title":"EC2Family Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/IAM_policy.html","text":"NOTE: Policy is only required to complete the exercise to create a cost optimization team. It can be delelted once the first exercise is complete. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:ListPolicies , iam:GetPolicyVersion , iam:CreateGroup , iam:GetPolicy , iam:DeletePolicy , iam:DetachGroupPolicy , iam:ListGroupPolicies , iam:AttachUserPolicy , iam:CreateUser , iam:GetGroup , iam:CreatePolicy , iam:CreateLoginProfile , iam:AddUserToGroup , iam:ListPolicyVersions , iam:AttachGroupPolicy , iam:ListUsers , iam:ListAttachedGroupPolicies , iam:ListGroups , iam:GetGroupPolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion , iam:GetLoginProfile ], Resource : * } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/Region_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : [ ec2:* , rds:* , s3:* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : us-east-1 }} } ] }","title":"Region Restrict"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html","text":"Level 200: Pricing Models http://wellarchitectedlabs.com Introduction In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework. Goals Perform a Reserved Instance analysis Filter and sort the recommendations Prerequisites An AWS Account Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#level-200-pricing-models","text":"http://wellarchitectedlabs.com","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#introduction","text":"In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#goals","text":"Perform a Reserved Instance analysis Filter and sort the recommendations","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#prerequisites","text":"An AWS Account Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#best-practice-checklist","text":"[ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html","text":"Level 200: Pricing Models Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab Feedback survey 1. View an RI report We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. NOTE : Analysis can ONLY be done on All Up Front recommendations, as this allows the break even to show when the entire RI term (12 or 36 months) is paid off. After the analysis is performed, you can proceed to purchase the required RI type. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option All upfront, Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable. 2. Download and prepare the RI CSV files 1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy : =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column): =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - Delete the following columns as they are not necessary: Recommendation Date , Owner Account , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Offering Class , Term , Payment Option , Upfront Cost , Recurring Monthly Cost . You should be left with the following columns: We now have the required data required to be able to analyze, and filter out the high risk and low return RIs. 3. Sort and filter the RI CSV files RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs. 3.1 Filter out low risk, and high return RIs 1 - To get the lowest risk, we sort by Break Even Months smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in less than 6months - so if they are used for 6 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Break Even Months of 7, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this: 3.2 Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of 80%. While this is reflected through the Break even (if utilization is low, break even would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls 3.3 Making recommendations We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible). 4. Teardown There are no resources or configuration items that are created during this workshop. 5. Rate this lab 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#level-200-pricing-models","text":"","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#table-of-contents","text":"View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#1-view-an-ri-report","text":"We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. NOTE : Analysis can ONLY be done on All Up Front recommendations, as this allows the break even to show when the entire RI term (12 or 36 months) is paid off. After the analysis is performed, you can proceed to purchase the required RI type. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option All upfront, Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable.","title":"1. View an RI report"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#2-download-and-prepare-the-ri-csv-files","text":"1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy : =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column): =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - Delete the following columns as they are not necessary: Recommendation Date , Owner Account , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Offering Class , Term , Payment Option , Upfront Cost , Recurring Monthly Cost . You should be left with the following columns: We now have the required data required to be able to analyze, and filter out the high risk and low return RIs.","title":"2. Download and prepare the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#3-sort-and-filter-the-ri-csv-files","text":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs.","title":"3. Sort and filter the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#31-filter-out-low-risk-and-high-return-ris","text":"1 - To get the lowest risk, we sort by Break Even Months smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in less than 6months - so if they are used for 6 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Break Even Months of 7, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this:","title":"3.1 Filter out low risk, and high return RIs"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#32-filter-out-usage-patterns","text":"It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of 80%. While this is reflected through the Break even (if utilization is low, break even would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls","title":"3.2 Filter out usage patterns"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#33-making-recommendations","text":"We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible).","title":"3.3 Making recommendations"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#4-teardown","text":"There are no resources or configuration items that are created during this workshop.","title":"4. Teardown"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html","text":"Level 200: Cost and Usage Analysis http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Have usage that is tagged (preferred, not mandatory) Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#level-200-cost-and-usage-analysis","text":"http://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#goals","text":"Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Have usage that is tagged (preferred, not mandatory)","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 200: Cost and Usage Analysis Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab Feedback survey 1. Verify your CUR files are being delivered We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. Workshop.sql October 2018 Usage November 2018 Usage December 2018 Usage 2. Use AWS Glue to enable access to CUR files via Amazon Athena We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Select data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create : 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Go to the Athena Console: 20 - Select the drop down arrow, and click on the new database: 21 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check - The database name is correct the same as the SQL file - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL. 3. Cost and Usage analysis We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 3.1 What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10; 3.2 Top Costs To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; 3.3 Tagging and Chargeback Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 3.4 Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc 4. Tear down Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab. 5. Rate this lab 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-200-cost-and-usage-analysis","text":"","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-verify-your-cur-files-are-being-delivered","text":"We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. Workshop.sql October 2018 Usage November 2018 Usage December 2018 Usage","title":"1. Verify your CUR files are being delivered "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-use-aws-glue-to-enable-access-to-cur-files-via-amazon-athena","text":"We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Select data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create : 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Go to the Athena Console: 20 - Select the drop down arrow, and click on the new database: 21 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check - The database name is correct the same as the SQL file - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.","title":"2. Use AWS Glue to enable access to CUR files via Amazon Athena "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-cost-and-usage-analysis","text":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018","title":"3. Cost and Usage analysis "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#31-what-data-is-available-in-the-cur-file","text":"We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10;","title":"3.1 What data is available in the CUR file?"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#32-top-costs","text":"To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10;","title":"3.2 Top Costs"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#33-tagging-and-chargeback","text":"Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20","title":"3.3 Tagging and Chargeback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#34-reserved-instance-on-demand-and-spot-usage","text":"To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc","title":"3.4 Reserved Instance, On Demand and Spot Usage"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab.","title":"4. Tear down "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"6. Survey "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html","text":"Level 200: Cost Visualization http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage Prerequisites A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#level-200-cost-visualization","text":"http://wellarchitectedlabs.com","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#goals","text":"Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html","text":"Level 200: Cost Visualization Authors Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab Feedback survey 1. Setup Amazon QuickSight The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console: 1.1 Setup QuickSight for the first time If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left: 2. Create a data set We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data. 3. Create visualizations We will now start to visualize our costs and usage, and create a dashboard. 3.1 Cost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter : 3.2 Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made. 3.3 Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible. 3.4 Dashboard Complete Your dashboard is now complete, you should have a similar dashboard to below: 4. Share your Analysis and Dashboard Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set. 4.1 Share an analysis To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.: 4.2 Publish a dashboard To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email: 5. Tear down It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution. 5.1 Cancel your QuickSight subscription Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe : 6. Rate this lab 7. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#level-200-cost-visualization","text":"","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#authors","text":"Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab Feedback survey","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#1-setup-amazon-quicksight","text":"The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console:","title":"1. Setup Amazon QuickSight "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#11-setup-quicksight-for-the-first-time","text":"If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left:","title":"1.1 Setup QuickSight for the first time"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#2-create-a-data-set","text":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data.","title":"2. Create a data set "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#3-create-visualizations","text":"We will now start to visualize our costs and usage, and create a dashboard.","title":"3. Create visualizations "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#31-cost-by-account-and-product","text":"The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter :","title":"3.1 Cost by account and product"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#32-elasticity","text":"The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made.","title":"3.2 Elasticity"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#33-cost-by-line-item-description","text":"The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.","title":"3.3 Cost by line item description"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#34-dashboard-complete","text":"Your dashboard is now complete, you should have a similar dashboard to below:","title":"3.4 Dashboard Complete"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#4-share-your-analysis-and-dashboard","text":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.","title":"4. Share your Analysis and Dashboard "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#41-share-an-analysis","text":"To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.:","title":"4.1 Share an analysis"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#42-publish-a-dashboard","text":"To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email:","title":"4.2 Publish a dashboard"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#5-tear-down","text":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#51-cancel-your-quicksight-subscription","text":"Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe :","title":"5.1 Cancel your QuickSight subscription"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#7-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazons Privacy Policy.","title":"7. Survey "},{"location":"Cost/Cost_and_Usage_Analysis/README.html","text":"AWS Well-Architected Cost Optimization Labs http://wellarchitectedlabs.com Cost and Usage Analysis Critical to Cost Optimization is cost and usage analysis. The first step in being able to optimize is understanding your usage, and therefore costs. After analysis is performed, you will gain insights and be able to modify your usage, leading to increasing optimization of your workloads. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Labs: 100 and 200 These labs are from the fundamentals series and must be completed before you move onto the 300 series. 100 #4 Cost and Usage Analysis 200 #4 Cost and Usage Analysis 300 These labs focus on specific items within cost and usage analysis and are used to achieve a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. 300 Billing Analysis - Automated CUR Updates and Ingestion 300 Billing Analysis - Multi Account CUR Access 300 Billing Analysis - Splitting and Sharing CUR Access","title":"Analysis Overview"},{"location":"Cost/Cost_and_Usage_Analysis/README.html#aws-well-architected-cost-optimization-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/Cost_and_Usage_Analysis/README.html#cost-and-usage-analysis","text":"Critical to Cost Optimization is cost and usage analysis. The first step in being able to optimize is understanding your usage, and therefore costs. After analysis is performed, you will gain insights and be able to modify your usage, leading to increasing optimization of your workloads. For more information about cost optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Cost and Usage Analysis"},{"location":"Cost/Cost_and_Usage_Analysis/README.html#labs","text":"","title":"Labs:"},{"location":"Cost/Cost_and_Usage_Analysis/README.html#100-and-200","text":"These labs are from the fundamentals series and must be completed before you move onto the 300 series. 100 #4 Cost and Usage Analysis 200 #4 Cost and Usage Analysis","title":"100 and 200"},{"location":"Cost/Cost_and_Usage_Analysis/README.html#300","text":"These labs focus on specific items within cost and usage analysis and are used to achieve a specific outcome. They must be done after the 100 and 200 labs, however they can be completed independantly of each other. 300 Billing Analysis - Automated CUR Updates and Ingestion 300 Billing Analysis - Multi Account CUR Access 300 Billing Analysis - Splitting and Sharing CUR Access","title":"300"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html","text":"Level 300: Automated CUR Updates and Ingestion http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework. Goals Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multilpe Cost and Usage Reports in the same bucket Prerequisites An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features Start the Lab! Best Practice Checklist [ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multilpe CURs in AWS Glue/Athena License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#level-300-automated-cur-updates-and-ingestion","text":"http://wellarchitectedlabs.com","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#introduction","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#goals","text":"Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multilpe Cost and Usage Reports in the same bucket","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#prerequisites","text":"An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#best-practice-checklist","text":"[ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multilpe CURs in AWS Glue/Athena","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html","text":"Level 300: Automated CUR Updates and Ingestion Authors Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create the CloudFormation stack Multiple CURs Tear down Rate this Lab Survey 1. Create the CloudFormation Stack This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports. 2. Multiple CURs This step is used when there are multilpe CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashbaord and verify that there is a single database, containing multilpe tables: 3. Tear down Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack : 4. Rate this lab 5. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#level-300-automated-cur-updates-and-ingestion","text":"","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#table-of-contents","text":"Create the CloudFormation stack Multiple CURs Tear down Rate this Lab Survey","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#1-create-the-cloudformation-stack","text":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.","title":"1. Create the CloudFormation Stack"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#2-multiple-curs","text":"This step is used when there are multilpe CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashbaord and verify that there is a single database, containing multilpe tables:","title":"2. Multiple CURs"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#3-tear-down","text":"Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack :","title":"3. Tear down"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#4-rate-this-lab","text":"","title":"4. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#5-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics) , so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"5. Survey "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html","text":"Level 300: Multi Account CUR Access http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost. Goals Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket Start the Lab! Best Practice Checklist [ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#level-300-multi-account-cur-access","text":"http://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#goals","text":"Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#permissions-required","text":"IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#best-practice-checklist","text":"[ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html","text":"Level 300: Multi Account CUR Access http://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab Survey 1. Access Master/Payer via a Role To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privliege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : sts:AssumeRole , Resource : arn:aws:iam::(Account ID):role/(Role name) } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would. 2. Access Master/Payer via a User You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena 3. Use Athena to access a CUR in Master/Payer You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 4. Tear down Execute either of these steps depending on the implementation you chose above. 4.1 Access Master/Payer via a Role 1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read 4.2 Access Master/Payer via a User 1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read 4.3 Use Athena to access a CUR in Master/Payer 1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didnt save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created 5. Rate this lab 6. Survey Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics), so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#level-300-multi-account-cur-access","text":"http://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#table-of-contents","text":"Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab Survey","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#1-access-masterpayer-via-a-role","text":"To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privliege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : sts:AssumeRole , Resource : arn:aws:iam::(Account ID):role/(Role name) } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would.","title":"1. Access Master/Payer via a Role  "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#2-access-masterpayer-via-a-user","text":"You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena","title":"2. Access Master/Payer via a User "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#3-use-athena-to-access-a-cur-in-masterpayer","text":"You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"3. Use Athena to access a CUR in Master/Payer "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#4-tear-down","text":"Execute either of these steps depending on the implementation you chose above.","title":"4. Tear down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#41-access-masterpayer-via-a-role","text":"1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read","title":"4.1 Access Master/Payer via a Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#42-access-masterpayer-via-a-user","text":"1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read","title":"4.2 Access Master/Payer via a User"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#43-use-athena-to-access-a-cur-in-masterpayer","text":"1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didnt save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created","title":"4.3 Use Athena to access a CUR in Master/Payer"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#6-survey","text":"Thanks for taking the lab, We hope that you can take this short survey ( 2 minutes), to share your insights and help us improve our content. This survey is hosted by an external company (Qualtrics), so the link above does not lead to our website. Please note that AWS will own the data gathered via this survey and will not share the information/results collected with survey respondents. Your responses to this survey will be subject to Amazon\u2019s Privacy Policy.","title":"6. Survey  "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , glue:GetCrawler , glue:GetDataCatalogEncryptionSettings , glue:GetTableVersions , glue:GetPartitions , athena:GetQueryResults , athena:ListWorkGroups , athena:GetNamedQuery , glue:GetDevEndpoint , glue:GetSecurityConfiguration , glue:GetResourcePolicy , glue:GetTrigger , glue:GetUserDefinedFunction , athena:GetExecutionEngine , glue:GetJobRun , athena:GetExecutionEngines , s3:HeadBucket , glue:GetUserDefinedFunctions , glue:GetClassifier , s3:PutAccountPublicAccessBlock , athena:GetQueryResultsStream , glue:GetJobs , glue:GetTables , glue:GetTriggers , athena:GetNamespace , athena:GetQueryExecutions , athena:GetCatalogs , athena:ListNamedQueries , athena:GetNamespaces , glue:GetPartition , glue:GetDevEndpoints , athena:GetTables , athena:GetTable , athena:BatchGetNamedQuery , athena:BatchGetQueryExecution , glue:GetJob , glue:GetConnections , glue:GetCrawlers , glue:GetClassifiers , athena:ListQueryExecutions , glue:GetCatalogImportStatus , athena:GetWorkGroup , glue:GetConnection , glue:BatchGetPartition , glue:GetSecurityConfigurations , glue:GetDatabases , athena:ListTagsForResource , glue:GetTable , glue:GetDatabase , s3:GetAccountPublicAccessBlock , glue:GetDataflowGraph , s3:ListAllMyBuckets , athena:GetQueryExecution , glue:GetPlan , glue:GetCrawlerMetrics , glue:GetJobRuns ], Resource : * }, { Sid : VisualEditor1 , Effect : Allow , Action : [ s3:PutObject , s3:GetObject , s3:ListBucketMultipartUploads , s3:AbortMultipartUpload , s3:CreateBucket , s3:ListBucket , s3:GetBucketLocation , s3:ListMultipartUploadParts ], Resource : [ arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1 , arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1/* ] }, { Sid : VisualEditor2 , Effect : Allow , Action : [ s3:ListBucketByTags , s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:GetInventoryConfiguration , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:GetBucketLogging , s3:ListBucket , s3:GetAccelerateConfiguration , s3:GetBucketPolicy , s3:GetObjectVersionTorrent , s3:GetObjectAcl , s3:GetEncryptionConfiguration , s3:GetBucketRequestPayment , s3:GetObjectVersionAcl , s3:GetObjectTagging , s3:GetMetricsConfiguration , s3:GetBucketPublicAccessBlock , s3:GetBucketPolicyStatus , s3:ListBucketMultipartUploads , s3:GetBucketWebsite , s3:GetBucketVersioning , s3:GetBucketAcl , s3:GetBucketNotification , s3:GetReplicationConfiguration , s3:ListMultipartUploadParts , s3:GetObject , s3:GetObjectTorrent , s3:GetBucketCORS , s3:GetAnalyticsConfiguration , s3:GetObjectVersionForReplication , s3:GetBucketLocation , s3:GetObjectVersion ], Resource : [ arn:aws:s3::: S3 CUR Bucket /* , arn:aws:s3::: S3 CUR Bucket ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { Version : 2008-10-17 , Id : Policy1335892530063 , Statement : [ { Sid : Stmt1335892150622 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : [ s3:GetBucketAcl , s3:GetBucketPolicy ], Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1335892526596 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : s3:PutObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* }, { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/crawler-cfn.html","text":"Below is a copy of the crawler config file. Modifications are between ' * ' characters. Variables that need to be changed below: (CUR Billing Bucket) (name): the account name of the Payer containing the CUR, this is the email excluding @companyname.com (name2): the account name of the linked account accessing the CUR (Canonical ID): the Canonical User ID for the Payer containing the CUR, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html (Canonical ID2): the Canonical User ID for the linked account accessing the CUR AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: 'athenacurcfn_workshop_c_u_r' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3:::(CUR Billing Bucket)* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' ***** - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' ***** AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3://(CUR Billing Bucket)' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); ***** const util = require('util'); ***** exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { ***** // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: '(CUR Billing Bucket)' ReportKey: 'cur/WorkshopCUR/WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3://(CUR Billing Bucket)/cur/WorkshopCUR/cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html","text":"Level 300: Splitting the CUR and Sharing Access http://wellarchitectedlabs.com Introduction This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script Goals Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation teamplates Create, save and execute Athena queries Create and run a Glue crawler Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#level-300-splitting-the-cur-and-sharing-access","text":"http://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#goals","text":"Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#permissions-required","text":"Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation teamplates Create, save and execute Athena queries Create and run a Glue crawler","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html","text":"Level 300: Splitting the CUR and Sharing Access http://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab 1. Setup Output S3 Bucket We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { Version : 2012-10-17 , Statement : [ { Sid : AllowListingOfFolders , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:ListBucket , Resource : arn:aws:s3:::(bucket) }, { Sid : AllowAllS3ActionsInSubFolder , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:* , Resource : arn:aws:s3:::(bucket)/(folder)/* } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:PutObjectVersionAcl , s3:PutObjectAcl ], Resource : arn:aws:s3:::(bucket name)/* } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account. 2. Perform one off Fill of Member/Linked Data Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM (database) . (table) where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table 3. Create Athena Saved Queries to Write new Data Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM (database) . (table) where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required. 4. Create Lambda function to run the Saved Queries This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashaboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime: 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top, you will need to change the arrays and Athena variable. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered. 5. Trigger the Lambda When a CUR is Delivered It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions. 6. Sub Account Crawler Setup The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 7. Tear Down We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab. 7.1 Sub Account 1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler 7.2 Master/Payer Account 1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder 8. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#level-300-splitting-the-cur-and-sharing-access","text":"http://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#table-of-contents","text":"Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#1-setup-output-s3-bucket","text":"We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { Version : 2012-10-17 , Statement : [ { Sid : AllowListingOfFolders , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:ListBucket , Resource : arn:aws:s3:::(bucket) }, { Sid : AllowAllS3ActionsInSubFolder , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:* , Resource : arn:aws:s3:::(bucket)/(folder)/* } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:PutObjectVersionAcl , s3:PutObjectAcl ], Resource : arn:aws:s3:::(bucket name)/* } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account.","title":"1. Setup Output S3 Bucket "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#2-perform-one-off-fill-of-memberlinked-data","text":"Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM (database) . (table) where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table","title":"2. Perform one off Fill of Member/Linked Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#3-create-athena-saved-queries-to-write-new-data","text":"Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM (database) . (table) where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required.","title":"3. Create Athena Saved Queries to Write new Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#4-create-lambda-function-to-run-the-saved-queries","text":"This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashaboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime: 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top, you will need to change the arrays and Athena variable. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered.","title":"4. Create Lambda function to run the Saved Queries "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#5-trigger-the-lambda-when-a-cur-is-delivered","text":"It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions.","title":"5. Trigger the Lambda When a CUR is Delivered "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#6-sub-account-crawler-setup","text":"The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"6. Sub Account Crawler Setup "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#7-tear-down","text":"We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab.","title":"7. Tear Down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#71-sub-account","text":"1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler","title":"7.1 Sub Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#72-masterpayer-account","text":"1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder","title":"7.2 Master/Payer Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , glue:GetCrawler , glue:GetDataCatalogEncryptionSettings , glue:GetTableVersions , glue:GetPartitions , athena:GetQueryResults , athena:ListWorkGroups , athena:GetNamedQuery , glue:GetDevEndpoint , glue:GetSecurityConfiguration , glue:GetResourcePolicy , glue:GetTrigger , glue:GetUserDefinedFunction , athena:GetExecutionEngine , glue:GetJobRun , athena:GetExecutionEngines , s3:HeadBucket , glue:GetUserDefinedFunctions , glue:GetClassifier , s3:PutAccountPublicAccessBlock , athena:GetQueryResultsStream , glue:GetJobs , glue:GetTables , glue:GetTriggers , athena:GetNamespace , athena:GetQueryExecutions , athena:GetCatalogs , athena:ListNamedQueries , athena:GetNamespaces , glue:GetPartition , glue:GetDevEndpoints , athena:GetTables , athena:GetTable , athena:BatchGetNamedQuery , athena:BatchGetQueryExecution , glue:GetJob , glue:GetConnections , glue:GetCrawlers , glue:GetClassifiers , athena:ListQueryExecutions , glue:GetCatalogImportStatus , athena:GetWorkGroup , glue:GetConnection , glue:BatchGetPartition , glue:GetSecurityConfigurations , glue:GetDatabases , athena:ListTagsForResource , glue:GetTable , glue:GetDatabase , s3:GetAccountPublicAccessBlock , glue:GetDataflowGraph , s3:ListAllMyBuckets , athena:GetQueryExecution , glue:GetPlan , glue:GetCrawlerMetrics , glue:GetJobRuns ], Resource : * }, { Sid : VisualEditor1 , Effect : Allow , Action : [ s3:PutObject , s3:GetObject , s3:ListBucketMultipartUploads , s3:AbortMultipartUpload , s3:CreateBucket , s3:ListBucket , s3:GetBucketLocation , s3:ListMultipartUploadParts ], Resource : [ arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1 , arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1/* ] }, { Sid : VisualEditor2 , Effect : Allow , Action : [ s3:ListBucketByTags , s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:GetInventoryConfiguration , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:GetBucketLogging , s3:ListBucket , s3:GetAccelerateConfiguration , s3:GetBucketPolicy , s3:GetObjectVersionTorrent , s3:GetObjectAcl , s3:GetEncryptionConfiguration , s3:GetBucketRequestPayment , s3:GetObjectVersionAcl , s3:GetObjectTagging , s3:GetMetricsConfiguration , s3:GetBucketPublicAccessBlock , s3:GetBucketPolicyStatus , s3:ListBucketMultipartUploads , s3:GetBucketWebsite , s3:GetBucketVersioning , s3:GetBucketAcl , s3:GetBucketNotification , s3:GetReplicationConfiguration , s3:ListMultipartUploadParts , s3:GetObject , s3:GetObjectTorrent , s3:GetBucketCORS , s3:GetAnalyticsConfiguration , s3:GetObjectVersionForReplication , s3:GetBucketLocation , s3:GetObjectVersion ], Resource : [ arn:aws:s3::: S3 CUR Bucket /* , arn:aws:s3::: S3 CUR Bucket ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3LinkedPutACL.html","text":"Here is the Lambda function to re-write object ACLs. It is triggered by an S3 Event, reads the folder from the object - and then applies the required object ACL: FULL_CONTROL for the owner, READ for the sub account. Edit the following fields in the code below: folder1 : The name of the folder where new files will be placed Owner Account Name : The owner account name - the account email without the @companyname, they will get FULL_CONTROL permissions Owner Canonical ID : The owner canonical ID, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html Sub Account Name : The sub account name - the account email without the @companyname, they will get READ permissions Sub Acct Canonical ID : The sub account canonical ID const AWS = require('aws-sdk'); const util = require('util'); // Permissions for the new objects // Key MUST match the top level folder // Format: owner account name - Canonical ID - sub account name - canonical ID // This will give owner full permission sub account read only permission var permissions = new Array(); var permissions = { ' folder1 ': [' owner acct name ',' Owner Canonical ID ',' sub account name ',' Sub Acct Canonical ID '], ' folder2 ': [' owner acct name ',' Owner Canonical ID ',' sub account name ',' Sub Acct Canonical ID '] }; // Main Loop exports.handler = function(event, context, callback) { // If its an object delete, do nothing if (event.RequestType === 'Delete') { } else // Its an object put { // Get the source bucket from the S3 event var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters, decode it var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // Gets the top level folder, which is the key for the permissions array var folderID = srcKey.split( / )[0]; // Define the object permissions, using the permissions array var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][2], 'ID': permissions[folderID][3] }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); } };","title":"S3LinkedPutACL"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { Version : 2008-10-17 , Id : Policy1335892530063 , Statement : [ { Sid : Stmt1335892150622 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : [ s3:GetBucketAcl , s3:GetBucketPolicy ], Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1335892526596 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : s3:PutObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* }, { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/SubAcctSplit_Role.html","text":"Review the policy below, and use it as a starting point to create your policy for the Lambda fuction. The following fields will need to be changed: Output bucket: The S3 bucket that will contain the output from the Athena queries Account ID: the master/payer account ID Source bucket: the location of the original CUR files in the master/payer { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , s3:DeleteObjectVersion , athena:GetQueryResults , s3:ListBucket , athena:GetNamedQuery , logs:PutLogEvents , athena:ListQueryExecutions , athena:ListNamedQueries , s3:PutObject , s3:GetObject , logs:CreateLogStream , athena:GetQueryExecution , s3:DeleteObject ], Resource : [ arn:aws:s3:::(output bucket)/* , arn:aws:logs:us-east-1:(account ID):log-group:/aws/lambda/SubAcctSplit:* , arn:aws:athena:*:*:workgroup/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : s3:ListBucket , Resource : arn:aws:s3:::* }, { Sid : VisualEditor2 , Effect : Allow , Action : [ glue:GetDatabase , glue:CreateTable , glue:GetPartitions , glue:DeleteTable , glue:GetTable ], Resource : * }, { Sid : VisualEditor3 , Effect : Allow , Action : [ s3:GetBucketLocation , s3:GetObject , s3:ListBucket , s3:ListBucketMultipartUploads , s3:ListMultipartUploadParts , s3:AbortMultipartUpload , s3:CreateBucket , s3:PutObject ], Resource : [ arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)/* , arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID) ] }, { Sid : VisualEditor4 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : arn:aws:s3:::(source bucket)/* }, { Sid : VisualEditor5 , Effect : Allow , Action : logs:CreateLogGroup , Resource : arn:aws:logs:us-east-1:(account ID):* } ] }","title":"SubAcctSplit Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/Sub_Account_Split.html","text":"Below is the code for the lambda function. You will need to modify the following variable: athena_output : This is where Athena puts output data, this is typically the master/payer Account ID, which is the default folder for Athena output queries output bucket : This is the output bucket for the Athena queries You will need to modify the following arrays, the order is important - the first folder in the subfolder array, will be given the permissions of the first element of the S3ObjectPolicies array. subfolders : This contains the list of folders that the queries write to S3ObjectPolicies : This contains the S3 Object permissions ACL that will be written to objects in the corresponding folder. You will need to add the owners details (master/payer account) and the grantee (sub account) details. import boto3 import json import datetime import time # Get the current date, so you know which months folder you're working on now = datetime.datetime.now() # Variables to construct the s3 folder name # YES! you can do multiple subfolders if you have multiple queries to run, 1 subfolder per query currentmonth = '/year_1=' + str(now.year) + '/month_1=' + str(now.month) + '/' bucketname = '(output bucket)' #List of Subfolders ACLs to apply to objects in them #There MUST be a 1:1 between subfolders policies subfolders = [' folder1 '] S3ObjectPolicies = ['ACL=\\'bucket-owner-full-control\\',AccessControlPolicy={\\'Grants\\':[{\\'Grantee\\': {\\'DisplayName\\': \\' account name \\',\\'ID\\': \\' canonical ID \\',\\'Type\\': \\'CanonicalUser\\'},\\'Permission\\': \\'FULL_CONTROL\\'}],\\'Owner\\':{\\'DisplayName\\':\\' account name \\',\\'ID\\':\\' canonical ID \\'}},Bucket=\\''+bucketname+'\\',Key=\\'objectkey\\''] # Arrays to hold the Athena delete create queries that we need to run delete_query_strings = [] create_query_strings = [] # Athena output folder athena_output = 's3://aws-athena-query-results-us-east-1- account ID /' # Main loop def lambda_handler(event, context): # Clear the current months S3 folder s3_clear_folders() # Get the athena queries to run get_athena_queries() # Make sure to delete any existing temp tables, so no wobbly's are thrown run_delete_athena_queries() # Create the athena tables, which will actually output data to S3 folders run_create_athena_queries() # Delete the array in case of another Lambda invocation create_query_strings.clear() # You could make another call to delete the tables, however you need to make sure # the creates are finished, which may take some time, consuming time($) in Lambda # run_delete_athena_queries() # Delete the array in case of another Lambda invocation delete_query_strings.clear() return { 'statusCode': 200, 'body': json.dumps('Finished!') } # Clear the S3 folders for the current month def s3_clear_folders(): # Get S3 client/object client = boto3.client('s3') # For each subfolder - in case you have multilpe subfolders, i.e. multilpe accounts/business units to split data out to for subfolder in subfolders: # List all objects in the current months bucket response = client.list_objects_v2( Bucket=bucketname, Prefix=subfolder + currentmonth ) # Get how many objects there are to delete, if any keys = response['KeyCount'] # Only try to delete if there's objects if (keys 0): # Get the ojbects from the response s3objects = response['Contents'] # For each object, we're going to delete it # cycle through the list of objects for s3object in s3objects: # Get the object key objectkey = s3object['Key'] # Delete the object response = client.delete_object( Bucket=bucketname, Key=objectkey ) # Get the Athena saved queries to run # They need to be labelled 'create_linked' or 'delete_linked' def get_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Get all the saved queries in Athena response = client.list_named_queries() # Get the named query IDs from the response named_query_IDs = response['NamedQueryIds'] # Go through all the query ID, to find the delete create queries we need to run for query_ID in named_query_IDs: # Get all the details of a named query using its ID named_query = client.get_named_query( NamedQueryId=query_ID ) # Get the query string query name of the query querystring = named_query['NamedQuery']['QueryString'] queryname = named_query['NamedQuery']['Name'] # If its a create query, add it to the list of create queries # We also replace the '/subfolder' string in the query with the folder structure for the current month if 'create_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('/subfolder', currentmonth) new_query2 = new_query1.replace('temp_table', 'temp_'+tableID) # Add the create query string to the array create_query_strings.append(new_query2) # If its a delete query, add it to the list of delete queries to execute later if 'delete_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('temp_table', 'temp_'+tableID) # Add the delete query string to the array delete_query_strings.append(new_query1) # Run the delete Athena queries to remove any temp tables def run_delete_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the delete query strings in the list for delete_query_string in delete_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=delete_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } ) # Get the state of the delete execution response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # A busy wait to make sure its finished before moving on # Tables must not exist before creation # If the function runs for a long time ($) you should implement step functions or a cost effective wait # This is a low cost of complexity solution while 'RUNNING' in response: # Busy wait to make sure it finishes time.sleep(1) # Get the current state of the query response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # Run the Athena queries to create the table populate the S3 data def run_create_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the create query strings in the list for create_query_string in create_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=create_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } )","title":"Sub Account Split"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/crawler-cfn.html","text":"Below is a sample crawler config file. It is suggested you modify your existing file, modifications are between '***' characters. Variables that need to be changed in the new code below: (region): The region that contains the Lambda function (accountID): The account that contains the Lambda function AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: '(Database Name)' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3::: bucketname / prefix / folder /WorkshopCUR* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' ***** - 'lambda:InvokeFunction' ***** Resource: ***** - 'arn:aws:logs:*:*:*' - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' ***** - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3:// bucket / prefix / folder /WorkshopCUR' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); ***** var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3::: bucket ' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3::: bucket ' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: ' bucket ' ReportKey: ' prefix / folder /WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3:// bucket / prefix / folder /cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Operations/README.html","text":"AWS Well-Architected Operational Excellence Labs http://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper. Labs: Operations Fundamentals Level 100: Introduction to Inventory and Patch Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Operations/README.html#aws-well-architected-operational-excellence-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Operational Excellence Labs"},{"location":"Operations/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.","title":"Introduction"},{"location":"Operations/README.html#labs","text":"","title":"Labs:"},{"location":"Operations/README.html#operations-fundamentals","text":"Level 100: Introduction to Inventory and Patch Management","title":"Operations Fundamentals"},{"location":"Operations/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html","text":"Level 100: Inventory and Patch Management http://wellarchitectedlabs.com Introduction In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management Goals: Automated deployment of infrastructure Dynamic management of resources Automated patch management Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#level-100-inventory-and-patch-management","text":"http://wellarchitectedlabs.com","title":"Level 100: Inventory and Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#introduction","text":"In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management","title":"Introduction"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#goals","text":"Automated deployment of infrastructure Dynamic management of resources Automated patch management","title":"Goals:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created.","title":"Prerequisites:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html","text":"Level 100: Inventory and Patch Management: Lab Guide In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: * Deployment of Infrastructure * Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. * Creating Maintenance Windows and Scheduling Automated Operations Activities * Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources 1. Setup Requirements You will need the following to be able to perform this lab: * Your own device for console access * An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs User and Group Management When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . IAM Users Groups As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it . 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group . 1.2 Log in to the AWS Management Console using your administrator account You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\ yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs. 1.3 Create an EC2 Key Pair Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab. 2. Deploy an Environment Using Infrastructure as Code Tagging We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: * Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types * Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization * Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. * Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words. Management Tools: CloudFormation AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments. 2.1 Deploy the Lab Infrastructure To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to http://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created. The impact of Infrastructure as Code With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency. 3. Inventory Management using Operations as Code Management Tools: Systems Manager AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . * You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS * The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents * Your EC2 instances must have outbound internet access * You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: * Amazon Linux base AMIs dated 2017.09 and later * Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments. 3.1 Setting up Systems Manager Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager. 3.2 Create a Second CloudFormation Stack Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod . Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated. 3.3 Using Systems Manager Inventory to Track Your Instances Under Insights in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top left corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager . Systems Manager: State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager. 3.4 Review Association Status Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Shared Resources in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Insights in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section. Systems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports. 4. Patch Management Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning * AWS does not test patches for Windows or Linux before making them available in Patch Manager . * If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments . Patch Baselines Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent. 4.1 Create a Patch Baseline Under Actions in the AWS Systems Manager navigation bar, choose Patch Manager . Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists but leave Product , Classification , and Severity at their default of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance level - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance level - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance level , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed. Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. * If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. * If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline. 4.2 Assign a Patch Group Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen. AWS-RunPatchBaseline AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems. AWS Systems Manager: Document An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities. 4.3 Examine AWS-RunPatchBaseline in Documents To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document. AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. 4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command Under Actions in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test . In the Command parameters section, leave the Operation value as the default Scan . The remaining Run Command features enable you to: * Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. * Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. * Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. * View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it. 4.5 Review Initial Patch Compliance Under Insights in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance Summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Corresponding managed instances list, you will see the individual compliance status and details. 4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command Under Actions in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , leave the default errors selected and specify 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted. 4.7 Review Patch Compliance After Patching Under Insights in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page, change the Compliance Type: to Patch . The Compliance Summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches. The Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity. Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities AWS Systems Manager: Maintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI. 5.1 Setting up Maintenance Windows Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { Version : 2012-10-17 , Statement :[ { Sid : , Effect : Allow , Principal :{ Service :[ ec2.amazonaws.com , ssm.amazonaws.com , sns.amazonaws.com ] }, Action : sts:AssumeRole } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group. Creating Maintenance Windows To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution. 5.2 Create a Patch Maintenance Window First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled . 5.3 Assigning Targets to Your Patch Maintenance Window After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags. 5.4 Assigning Tasks to Your Patch Maintenance Window After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page. 5.5 Review Maintenance Window Execution After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document . Bonus Content: Creating a Simple Notification Service Topic Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic. 6.1 Create and Subscribe to an SNS Topic To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user. 7 Removing Lab Resources Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier. 7.1 Remove resources created with CloudFormation Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#level-100-inventory-and-patch-management-lab-guide","text":"In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: * Deployment of Infrastructure * Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. * Creating Maintenance Windows and Scheduling Automated Operations Activities * Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources","title":"Level 100: Inventory and Patch Management: Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#1-setup","text":"","title":"1. Setup"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#requirements","text":"You will need the following to be able to perform this lab: * Your own device for console access * An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs","title":"Requirements"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#user-and-group-management","text":"When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"User and Group Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#iam-users-groups","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it .","title":"IAM Users &amp; Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group .","title":"1.1 Create Administrator IAM User and Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#12-log-in-to-the-aws-management-console-using-your-administrator-account","text":"You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\ yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs.","title":"1.2 Log in to the AWS Management Console using your administrator account"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#13-create-an-ec2-key-pair","text":"Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab.","title":"1.3 Create an EC2 Key Pair"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#2-deploy-an-environment-using-infrastructure-as-code","text":"","title":"2. Deploy an Environment Using Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#tagging","text":"We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: * Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types * Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization * Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. * Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words.","title":"Tagging"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-cloudformation","text":"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments.","title":"Management Tools: CloudFormation"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#21-deploy-the-lab-infrastructure","text":"To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to http://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created.","title":"2.1 Deploy the Lab Infrastructure"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-infrastructure-as-code","text":"With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency.","title":"The impact of Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#3-inventory-management-using-operations-as-code","text":"","title":"3. Inventory Management using Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-systems-manager","text":"AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . * You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS * The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents * Your EC2 instances must have outbound internet access * You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: * Amazon Linux base AMIs dated 2017.09 and later * Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments.","title":"Management Tools: Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#31-setting-up-systems-manager","text":"Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager.","title":"3.1 Setting up Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#32-create-a-second-cloudformation-stack","text":"Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod .","title":"3.2 Create a Second CloudFormation Stack"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-inventory","text":"You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.","title":"Systems Manager: Inventory"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#33-using-systems-manager-inventory-to-track-your-instances","text":"Under Insights in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top left corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager .","title":"3.3 Using Systems Manager Inventory to Track Your Instances"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-state-manager","text":"In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager.","title":"Systems Manager: State Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#34-review-association-status","text":"Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Shared Resources in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Insights in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section.","title":"3.4 Review Association Status"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-compliance","text":"You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.","title":"Systems Manager: Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#4-patch-management","text":"","title":"4. Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-patch-manager","text":"AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning * AWS does not test patches for Windows or Linux before making them available in Patch Manager . * If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments .","title":"Systems Manager: Patch Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-baselines","text":"Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent.","title":"Patch Baselines"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#41-create-a-patch-baseline","text":"Under Actions in the AWS Systems Manager navigation bar, choose Patch Manager . Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists but leave Product , Classification , and Severity at their default of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance level - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance level - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance level , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed.","title":"4.1 Create a Patch Baseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-groups","text":"A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. * If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. * If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline.","title":"Patch Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#42-assign-a-patch-group","text":"Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen.","title":"4.2 Assign a Patch Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-runpatchbaseline","text":"AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.","title":"AWS-RunPatchBaseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-document","text":"An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.","title":"AWS Systems Manager: Document"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#43-examine-aws-runpatchbaseline-in-documents","text":"To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document.","title":"4.3 Examine AWS-RunPatchBaseline in Documents"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-run-command","text":"AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.","title":"AWS Systems Manager: Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#44-scan-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Actions in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test . In the Command parameters section, leave the Operation value as the default Scan . The remaining Run Command features enable you to: * Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. * Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. * Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. * View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it.","title":"4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#45-review-initial-patch-compliance","text":"Under Insights in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance Summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Corresponding managed instances list, you will see the individual compliance status and details.","title":"4.5 Review Initial Patch Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#46-patch-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Actions in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , leave the default errors selected and specify 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted.","title":"4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#47-review-patch-compliance-after-patching","text":"Under Insights in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page, change the Compliance Type: to Patch . The Compliance Summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches.","title":"4.7 Review Patch Compliance After Patching"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-operations-as-code","text":"In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity.","title":"The Impact of Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-maintenance-windows-and-scheduling-automated-operations-activities","text":"","title":"Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-maintenance-windows","text":"AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI.","title":"AWS Systems Manager: Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#51-setting-up-maintenance-windows","text":"Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { Version : 2012-10-17 , Statement :[ { Sid : , Effect : Allow , Principal :{ Service :[ ec2.amazonaws.com , ssm.amazonaws.com , sns.amazonaws.com ] }, Action : sts:AssumeRole } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group.","title":"5.1 Setting up Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#creating-maintenance-windows","text":"To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution.","title":"Creating Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#52-create-a-patch-maintenance-window","text":"First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled .","title":"5.2 Create a Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#53-assigning-targets-to-your-patch-maintenance-window","text":"After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags.","title":"5.3 Assigning Targets to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#54-assigning-tasks-to-your-patch-maintenance-window","text":"After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page.","title":"5.4 Assigning Tasks to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#55-review-maintenance-window-execution","text":"After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document .","title":"5.5 Review Maintenance Window Execution"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-a-simple-notification-service-topic","text":"Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.","title":"Bonus Content: Creating a Simple Notification Service Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#61-create-and-subscribe-to-an-sns-topic","text":"To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user.","title":"6.1 Create and Subscribe to an SNS Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#7-removing-lab-resources","text":"Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier.","title":"7 Removing Lab Resources"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#71-remove-resources-created-with-cloudformation","text":"Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"7.1 Remove resources created with CloudFormation"},{"location":"Performance/README.html","text":"AWS Well-Architected Performance Efficiency Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about performance efficiency on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected performance efficiency whitepaper. Labs: Coming soon...","title":"Coming Soon"},{"location":"Performance/README.html#aws-well-architected-performance-efficiency-labs","text":"","title":"AWS Well-Architected Performance Efficiency Labs"},{"location":"Performance/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about performance efficiency on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected performance efficiency whitepaper.","title":"Introduction"},{"location":"Performance/README.html#labs","text":"Coming soon...","title":"Labs:"},{"location":"Reliability/README.html","text":"AWS Well-Architected Reliability Labs http://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper . Labs: Level 200: Testing for Resiliency of EC2 Level 300: Testing for Resiliency of EC2, RDS, and S3 License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Reliability/README.html#aws-well-architected-reliability-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Reliability Labs"},{"location":"Reliability/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .","title":"Introduction"},{"location":"Reliability/README.html#labs","text":"Level 200: Testing for Resiliency of EC2 Level 300: Testing for Resiliency of EC2, RDS, and S3","title":"Labs:"},{"location":"Reliability/README.html#license","text":"","title":"License"},{"location":"Reliability/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html","text":"Level 200: Testing for Resiliency of EC2 http://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#level-200-testing-for-resiliency-of-ec2","text":"http://wellarchitectedlabs.com","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#license","text":"","title":"License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html","text":"Level 200: Testing for Resiliency of EC2 Authors Rodney Lester, Reliability Lead, Well-Architected Table of Contents Discussion and Example Failure Scenarios Failure Modes Tear Down 1. Discussion and Example Failure Scenarios Please note a prerequisite to this lab is that you have deployed the static web application stack in the lab Automated Deployment of EC2 Web Application with the default parameters and recommended stack name. This step will test the web application and it's EC2 components inside the VPC you have created previously. There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections. 1.1 Setting Up the bash Environment All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to the region you have deployed your web app into and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: The AWS Region you have deployed the web app Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the 300 - Testing for Resiliency of EC2, RDS, and S3/Code/ directory. 1.2 Setting up a Programming Language Based Environment You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2 1.3 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 1.4 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs. 1.5 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 1.6 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 2. Failure Modes 2.1 EC2 Failure Mode The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group. 3. Tear down this lab The following instructions will remove the resources that you have created in this lab. There are no new resources to remove in this lab. References useful resources: AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#level-200-testing-for-resiliency-of-ec2","text":"","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#table-of-contents","text":"Discussion and Example Failure Scenarios Failure Modes Tear Down","title":"Table of Contents"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#1-discussion-and-example-failure-scenarios","text":"Please note a prerequisite to this lab is that you have deployed the static web application stack in the lab Automated Deployment of EC2 Web Application with the default parameters and recommended stack name. This step will test the web application and it's EC2 components inside the VPC you have created previously. There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections.","title":"1. Discussion and Example Failure Scenarios "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#11-setting-up-the-bash-environment","text":"All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to the region you have deployed your web app into and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: The AWS Region you have deployed the web app Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the 300 - Testing for Resiliency of EC2, RDS, and S3/Code/ directory.","title":"1.1 Setting Up the bash Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#12-setting-up-a-programming-language-based-environment","text":"You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2","title":"1.2 Setting up a Programming Language Based Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#13-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"1.3 Setting Up the Python Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#14-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs.","title":"1.4 Setting Up the Java Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#15-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"1.5 Setting Up the C# Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#16-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"1.6 Setting up the Powershell Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#2-failure-modes","text":"","title":"2. Failure Modes "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#21-ec2-failure-mode","text":"The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group.","title":"2.1 EC2 Failure Mode"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. There are no new resources to remove in this lab.","title":"3. Tear down this lab "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html","text":"Level 300: Testing for Resiliency of EC2, RDS, and S3 http://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document. Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview: Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#level-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"http://wellarchitectedlabs.com","title":"Level 300: Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#overview","text":"Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables.","title":"Overview:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html","text":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in. Prerequisites An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Building and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda. Building and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy. The Bootstrapping Script The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs. The SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses. Deploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it. CloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#builders-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#introduction","text":"This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#prerequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON","title":"Prerequisites"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-aws-lambda-functions","text":"Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip","title":"Building and uploading the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#debugging-the-aws-lambda-functions","text":"The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.","title":"Debugging the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-web-application","text":"The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.","title":"Building and Uploading the Web Application"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-bootstrapping-script","text":"The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.","title":"The Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-sql-in-the-bootstrapping-script","text":"The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.","title":"The SQL in the Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#deploying-the-state-machine","text":"The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.","title":"Deploying the State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#cloudformation-templates","text":"The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"CloudFormation templates"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html","text":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3 Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Table of Contents Deploying the infrastructure Discussion and Example Failure Scenarios Failure modes Tear Down 1. Deploying the infrastructure AWS requires \u201cService-Linked\u201d Roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. In the past, these Roles may have been created automatically for you, or we may need to create them. Here is how to find which roles you need to create. 1.1 Checking for Existing Service-Linked Roles Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for \u201cAutoScaling,\u201d \u201cELB\u201d or \u201cElasticLoadBalancing,\u201d and \u201cRDS.\u201d In this screenshot, the service linked role for AutoScaling exists, but the roles for ELB and RDS do not. Note which roles will need to be created as you will use this information when performing the next step. In the AWS Services Search Box, type \u201cCloudFormation\u201d and click enter. You will need to download the CloudFormation template that will deploy the lambda functions and step functions state machine. Change the region to Ohio and navigate to the CloudFormation console. On the CloudFormation console, click \u201cCreate Stack:\u201d. There are two versions that can be deployed. You can deploy in one AWS region, which will allow you start testing sooner, or you can deploy into 2 AWS regions, which will enable you to test some additional aspects of S3, as well as as simulation a regional failure of your application For a single region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy.json For a two region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy_two_regions.json Click the \u201cNext\u201d button. On this page you will enter the following information: For the single region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! For the two region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! DMSLambdaKey: \u201cReliability/DMSLambda.zip\u201d -Case sensitive! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! RDSRRLambdaKey: \"Reliability/RDSReadReplicaLambda.zip\" -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! Click the \u201cNext\u201d button. On the \u201cOptions\u201d page, click the \u201cNext\u201d button at the bottom of the page. On the \u201cReview\u201d page, scroll to the bottom and select the option \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate\u201d button. This will take you to the summary with the stack creation in progress. This will take approximately a minute to deploy. You now need to navigate to the Step Functions console. At the top of the window, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cStep Functions\u201d in the search box and press the enter key. On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. For input to the execution name, use \u201cBuildResiliency.\u201d One Region Deployment: For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } b. Then click the \u201cStart Execution\u201d button. ii. Two Region Deployment: a. For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { region1 : { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg }, region2 : { log_level : DEBUG , region_name : us-west-2 , secondary_region_name : us-east-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } } b. Then click the \u201cStart Execution\u201d button. 14. This will take approximately 20-25 minutes for one region to deploy and approximately 45-50 minutes for two regions to deploy. You will have enough to start executing the lab exercises for the two region in 25-30 minutes. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. 1. One Region: 1. Two Regions: 15. You can also watch the CloudFormation stacks as they are created. If you are in a workshop, the instructor will have some background information to share while this is created. You can resume testing when the web tier has been deployed in the Ohio region. This will look something like this on the visual workflow. 16. You can start testing: 1. When the WaitForWebApp step for a single region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 1. When the WaitForWebApp1 step for a two region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 17. Click the value and it will bring up the website: 2. Discussion and Example Failure Scenarios There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections. 2.1 Setting Up the bash Environment All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to \u201cus-east-2\u201d and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the Code/FailureSimulations/bash/ directory. 2.2 Setting up a Programming Language Based Environment You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2 2.3 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 2.4 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs. 2.5 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.6 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. 3. Failure modes 3.1 EC2 Failure Mode The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group. 3.2 RDS Failure Mode From the AWS EC2 Console (you will still need the VPC ID from the VPC Console), click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cRDS\u201d in the search box and press the enter key. Use this as the command line argument to the scripts/programs below. RDS Instance Failure in bash: Execute the failure mode script for failing over an RDS instance: $./failover_rds.sh vpc-id RDS Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_rds.py vpc-id RDS Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar RDS vpc-id RDS Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency RDS vpc-id RDS Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\failover_rds.ps1 vpc-id Watch the behavior of the website. What happens? Watch the EC2 Load Balancer Target Group and its Targets\u2019 health. Do the instances recover? 3.3 AZ Failure Availability zone failure. AZ Failure in bash: Execute the following script for failing over an Availability Zone: $./fail_az.sh az vpc-id AZ Failure in Python: Execute the failure mode script for failing an instance: $ python fail_az.py vpc-id az AZ Failure in Java: Execute the failure mode script for failing an instance: $ java -jar app-resiliency-1.0.jar AZ vpc-id az AZ Failure in C#: Execute the failure mode script for failing an instance: $ .\\AppResiliency AZ vpc-id az AZ Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_az.ps1 az vpc-id What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes? 3.4 Failure of S3 Failure of S3 means that the image will not be available. Failure in bash: The bash commands available do not allow for modification of the access permissions, so you'll have to do the work in the console. Navigate to the S3 console: [https://s3.console.aws.amazon.com/s3/home](https://s3.console.aws.amazon.com/s3/home] Select the bucket where the image is located. In my example, this is the bucket \"arc327-well-architected-for-reliability\" Select the object, then select the \"Permissions\" tab: Select the \"Public Access\" radio button, and deselect the \"Read object\" box: What is the expected effect? How long does it take to take effect? How would you diagnose if this is a larger problem than permissions? 3.5 Looking for more to do? You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal their failure modes. 1. Remove the network ACLs they added 2. Reconfigure the AutoScaling Groups to use the AZ 4. Tear down this lab In order to take down the lab environment, you will need to remove the association of the Network ACL that was added to fail the AZ, and delete the Network ACL in the console. You can then delete the CloudFormation stacks (in both AWS regions) that were created. Here are the instructions for how to do that. 1. Navigate to the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then press enter. On the VPC Console, click on Network ACLs and select the Network ACL that is associated with 4 subnets in the ResiliencyVPC. Click the \u201cSubnet Associations\u201d tab and then Click the \u201cEdit button. You need to associate the 2 subnets that were changed to the blocking Network ACL back to this default Network ACL. Select the 2 subnets available, then click the \u201cSave\u201d button. Now select the Network ACL that is associated with 0 Subnets and click the \u201cDelete\u201d button to delete this Network ACL. If you have deployed into 2 regions, you have to delete the Database Migration Service and RDS Read Replicas before you can delete the RDS instances. You can delete the DMS (in Oregon) and read replica at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cDMSforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDeleteStack:\u201d. You can then select the \u201c MySQLReadReplicaResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS Read Replica instance. If you have deployed in 2 regions, Navigate to the other region (Ohio, if you started in Oregon, and Oregon, if you started in Ohio), and perform the same deletion steps above for the RDS read replica and DMS, if in Oregon. Wait for the read replicas to be deleted. You can delete the web servers and RDS at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cWebServersforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDelete Stack:\u201d. You can then select the \u201c MySQLforResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS instance. You can then select the \u201cDeployResiliencyWorkshop\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the deployment machine and Lambda functions. Once all these stacks have deleted, you can then select the \u201cResiliencyVPC\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to delete the VPC and remove the last piece of infrastructure. When it is delete complete, you are done. References useful resources: License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#level-300-performing-resiliency-testing-for-ec2-rds-and-s3","text":"","title":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS","title":"Authors"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#table-of-contents","text":"Deploying the infrastructure Discussion and Example Failure Scenarios Failure modes Tear Down","title":"Table of Contents"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#1-deploying-the-infrastructure","text":"AWS requires \u201cService-Linked\u201d Roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. In the past, these Roles may have been created automatically for you, or we may need to create them. Here is how to find which roles you need to create.","title":"1. Deploying the infrastructure "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#11-checking-for-existing-service-linked-roles","text":"Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for \u201cAutoScaling,\u201d \u201cELB\u201d or \u201cElasticLoadBalancing,\u201d and \u201cRDS.\u201d In this screenshot, the service linked role for AutoScaling exists, but the roles for ELB and RDS do not. Note which roles will need to be created as you will use this information when performing the next step. In the AWS Services Search Box, type \u201cCloudFormation\u201d and click enter. You will need to download the CloudFormation template that will deploy the lambda functions and step functions state machine. Change the region to Ohio and navigate to the CloudFormation console. On the CloudFormation console, click \u201cCreate Stack:\u201d. There are two versions that can be deployed. You can deploy in one AWS region, which will allow you start testing sooner, or you can deploy into 2 AWS regions, which will enable you to test some additional aspects of S3, as well as as simulation a regional failure of your application For a single region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy.json For a two region deployment, select the option to \u201cSpecify an Amazon S3 template\" and enter https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/lambda_functions_for_deploy_two_regions.json Click the \u201cNext\u201d button. On this page you will enter the following information: For the single region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! For the two region deployment: Stack name: \u201cDeployResiliencyWorkshop\u201d -No spaces! DMSLambdaKey: \u201cReliability/DMSLambda.zip\u201d -Case sensitive! EnableAutoScalingServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be false since it already exists. EnableELBServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. EnableRDSServiceRole: \u201cfalse\u201d or \u201ctrue,\u201d depending on whether it exists. In the example, this will be true because it does not exist. LambdaFunctionsBucket: \u201caws-well-architected-labs-ohio\u201d -Case sensitive! RDSLambdaKey: \u201cReliability/RDSLambda.zip\u201d -Case sensitive! RDSRRLambdaKey: \"Reliability/RDSReadReplicaLambda.zip\" -Case sensitive! VPCLambdaKey: \u201cReliability/VPCLambda.zip\u201d -Case sensitive! WaitForStackLambdaKey: \u201cReliability/WaitForStack.zip\u201d -Case sensitive! WebAppLambdaKey: \u201cReliability/WebAppLambda.zip\u201d -Case sensitive! Click the \u201cNext\u201d button. On the \u201cOptions\u201d page, click the \u201cNext\u201d button at the bottom of the page. On the \u201cReview\u201d page, scroll to the bottom and select the option \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate\u201d button. This will take you to the summary with the stack creation in progress. This will take approximately a minute to deploy. You now need to navigate to the Step Functions console. At the top of the window, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cStep Functions\u201d in the search box and press the enter key. On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. For input to the execution name, use \u201cBuildResiliency.\u201d One Region Deployment: For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } b. Then click the \u201cStart Execution\u201d button. ii. Two Region Deployment: a. For Input, use the following: Note: If you want to test failure of S3, then you should use an image in S3 that you control, and it should have public read access only. { region1 : { log_level : DEBUG , region_name : us-east-2 , secondary_region_name : us-west-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg }, region2 : { log_level : DEBUG , region_name : us-west-2 , secondary_region_name : us-east-2 , cfn_region : us-east-2 , cfn_bucket : aws-well-architected-labs-ohio , folder : Reliability/ , workshop : 300-ResiliencyofEC2RDSandS3 , boot_bucket : aws-well-architected-labs-ohio , boot_prefix : Reliability/ , boot_object : bootstrap300Resiliency.sh , websiteimage : https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg } } b. Then click the \u201cStart Execution\u201d button. 14. This will take approximately 20-25 minutes for one region to deploy and approximately 45-50 minutes for two regions to deploy. You will have enough to start executing the lab exercises for the two region in 25-30 minutes. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. 1. One Region: 1. Two Regions: 15. You can also watch the CloudFormation stacks as they are created. If you are in a workshop, the instructor will have some background information to share while this is created. You can resume testing when the web tier has been deployed in the Ohio region. This will look something like this on the visual workflow. 16. You can start testing: 1. When the WaitForWebApp step for a single region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 1. When the WaitForWebApp1 step for a two region deployment is completed, return to the CloudFormation console and select the \u201cWebServersforResiliencyTesting\u201d stack and then the Outputs tab at bottom. 17. Click the value and it will bring up the website:","title":"1.1 Checking for Existing Service-Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#2-discussion-and-example-failure-scenarios","text":"There is a choice of environments to execute the failure simulations in. Linux command line (bash), Python, Java, and C#. The instructions for each environment are in separate sections.","title":"2. Discussion and Example Failure Scenarios "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#21-setting-up-the-bash-environment","text":"All the command line scripts use a utility called jq. You can download it from the site and leave it in your local directory, as long as that is in your execution path: https://stedolan.github.io/jq/ 1. You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. In Amazon linux, this is typically /home/ec2-user/bin. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Install the AWS Command Line Interface (CLI) if you do not have it installed (it is installed by default on Amazon Linux). https://aws.amazon.com/cli/ Run the aws configure command to configure your command line options. This will prompt you for the AWS Access Key ID, AWS Secret Access Key, and default region name. Enter the key information if you do not already have them installed, and set the default region to \u201cus-east-2\u201d and leave the default output format as \u201cNone.\u201d. $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: return Download the zip file of the resiliency bash scripts at the following URL: https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/bashresiliency.zip Unzip the folder in a location convenient for you to execute the scripts. They are also available in the Code/FailureSimulations/bash/ directory.","title":"2.1 Setting Up the bash Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#22-setting-up-a-programming-language-based-environment","text":"You will need the same files that the AWS command line uses for credentials. You can either install the command line and use the \u2018aws configure\u2019 command as outlined in the bash set up, or you can manually create the configuration files. To create the files manually, create a .aws folder/directory in your home directory. 1. Bash and powershell use the same command. mkdir ~/.aws Change directory to that directory to create the configuration file. Bash cd ~/.aws Powershell cd ~\\.aws Use a text editor (vim, emacs, notepad, wordpad) to create a text file (no extension named \u201ccredentials.\u201d In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key region = us-east-2","title":"2.2 Setting up a Programming Language Based Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#23-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the zip file of the resiliency scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/pythonresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"2.3 Setting Up the Python Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#24-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. In Amazon Linux, you need to install Java 8 and remove Java 7. $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip . Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.4 Setting Up the Java Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#25-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.5 Setting Up the C# Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#26-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Follow the \u201cGetting Started\u201d instructions for configuring credentials. https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-started.html Download the zipfile of the scripts at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/powershellresiliency.zip Unzip the folder in a location convenient for you to execute the scripts.","title":"2.6 Setting up the Powershell Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#3-failure-modes","text":"","title":"3. Failure modes "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#31-ec2-failure-mode","text":"The first failure mode will be to fail a web server. To prepare for this, you should have two consoles open: VPC and EC2. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cEC2\u201d in the search box and press the enter key. You also need the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then right click on the \u201cVPC Isolate Cloud Resources\u201d text and open the link in a new tab or window. You can then click the upward facing icon to the right of the word \u201cServices\u201d to make the menu of services disappear. On the EC2 Console, click \u201cInstances\u201d on the left side to bring up the list of instances. Use this as the command line argument to the scripts/programs below. Instance Failure in bash Execute the failure mode script for failing an instance: $./fail_instance.sh vpc-id Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_instance.py vpc-id Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar EC2 vpc-id Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency EC2 vpc-id Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_instance.ps1 vpc-id Watch the behavior of the Load Balancer Target Group and its Targets in the EC2 Console. See it get marked unhealthy and replaced by the Auto Scaling Group.","title":"3.1 EC2 Failure Mode"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#32-rds-failure-mode","text":"From the AWS EC2 Console (you will still need the VPC ID from the VPC Console), click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cRDS\u201d in the search box and press the enter key. Use this as the command line argument to the scripts/programs below. RDS Instance Failure in bash: Execute the failure mode script for failing over an RDS instance: $./failover_rds.sh vpc-id RDS Instance Failure in Python: Execute the failure mode script for failing an instance: $ python fail_rds.py vpc-id RDS Instance Failure in Java: Execute the failure mode program for failing an instance: $ java -jar app-resiliency-1.0.jar RDS vpc-id RDS Instance Failure in C#: Execute the failure mode program for failing an instance: $ .\\AppResiliency RDS vpc-id RDS Instance Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\failover_rds.ps1 vpc-id Watch the behavior of the website. What happens? Watch the EC2 Load Balancer Target Group and its Targets\u2019 health. Do the instances recover?","title":"3.2 RDS Failure Mode"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#33-az-failure","text":"Availability zone failure. AZ Failure in bash: Execute the following script for failing over an Availability Zone: $./fail_az.sh az vpc-id AZ Failure in Python: Execute the failure mode script for failing an instance: $ python fail_az.py vpc-id az AZ Failure in Java: Execute the failure mode script for failing an instance: $ java -jar app-resiliency-1.0.jar AZ vpc-id az AZ Failure in C#: Execute the failure mode script for failing an instance: $ .\\AppResiliency AZ vpc-id az AZ Failure in Powershell: Execute the failure mode script for failing an instance: $ .\\fail_az.ps1 az vpc-id What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes?","title":"3.3 AZ Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#34-failure-of-s3","text":"Failure of S3 means that the image will not be available. Failure in bash: The bash commands available do not allow for modification of the access permissions, so you'll have to do the work in the console. Navigate to the S3 console: [https://s3.console.aws.amazon.com/s3/home](https://s3.console.aws.amazon.com/s3/home] Select the bucket where the image is located. In my example, this is the bucket \"arc327-well-architected-for-reliability\" Select the object, then select the \"Permissions\" tab: Select the \"Public Access\" radio button, and deselect the \"Read object\" box: What is the expected effect? How long does it take to take effect? How would you diagnose if this is a larger problem than permissions?","title":"3.4 Failure of S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#35-looking-for-more-to-do","text":"You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal their failure modes. 1. Remove the network ACLs they added 2. Reconfigure the AutoScaling Groups to use the AZ","title":"3.5 Looking for more to do?"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#4-tear-down-this-lab","text":"In order to take down the lab environment, you will need to remove the association of the Network ACL that was added to fail the AZ, and delete the Network ACL in the console. You can then delete the CloudFormation stacks (in both AWS regions) that were created. Here are the instructions for how to do that. 1. Navigate to the VPC Console. From the AWS Console, click the downward facing icon to the right of the word \u201cServices.\u201d This will bring up the list of services. Type \u201cVPC\u201d in the search box, then press enter. On the VPC Console, click on Network ACLs and select the Network ACL that is associated with 4 subnets in the ResiliencyVPC. Click the \u201cSubnet Associations\u201d tab and then Click the \u201cEdit button. You need to associate the 2 subnets that were changed to the blocking Network ACL back to this default Network ACL. Select the 2 subnets available, then click the \u201cSave\u201d button. Now select the Network ACL that is associated with 0 Subnets and click the \u201cDelete\u201d button to delete this Network ACL. If you have deployed into 2 regions, you have to delete the Database Migration Service and RDS Read Replicas before you can delete the RDS instances. You can delete the DMS (in Oregon) and read replica at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cDMSforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDeleteStack:\u201d. You can then select the \u201c MySQLReadReplicaResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS Read Replica instance. If you have deployed in 2 regions, Navigate to the other region (Ohio, if you started in Oregon, and Oregon, if you started in Ohio), and perform the same deletion steps above for the RDS read replica and DMS, if in Oregon. Wait for the read replicas to be deleted. You can delete the web servers and RDS at the same time, but the console only allows you to select one of the stacks at a time. Select the \u201cWebServersforResiliencyTesting\u201d stack, then click the \u201cActions\u201d button, and click \u201cDelete Stack:\u201d. You can then select the \u201c MySQLforResiliencyTesting\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the RDS instance. You can then select the \u201cDeployResiliencyWorkshop\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to simultaneously delete the deployment machine and Lambda functions. Once all these stacks have deleted, you can then select the \u201cResiliencyVPC\u201d stack, click the \u201cActions\u201d button, and click \u201cDelete Stack\u201d to delete the VPC and remove the last piece of infrastructure. When it is delete complete, you are done.","title":"4. Tear down this lab "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#references-useful-resources","text":"","title":"References &amp; useful resources:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"Security/README.html","text":"AWS Well-Architected Security Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper . Labs: Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Basic EC2 with WAF Protection Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response with AWS Console and CLI Quests: Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity access management, detective controls, infrastructure protection, data protection and incident response. Identity Access Management This guide will help you improve your security in the AWS Well-Architetced area of Identity Access Management . License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Security/README.html#aws-well-architected-security-labs","text":"","title":"AWS Well-Architected Security Labs"},{"location":"Security/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper .","title":"Introduction"},{"location":"Security/README.html#labs","text":"Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Basic EC2 with WAF Protection Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response with AWS Console and CLI","title":"Labs:"},{"location":"Security/README.html#quests","text":"Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity access management, detective controls, infrastructure protection, data protection and incident response. Identity Access Management This guide will help you improve your security in the AWS Well-Architetced area of Identity Access Management .","title":"Quests:"},{"location":"Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/README.html","text":"Level 100: AWS Account and Root User Introduction This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting AWS credentials Fine-grained authorization Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: AWS Account and Root User"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#level-100-aws-account-and-root-user","text":"","title":"Level 100: AWS Account and Root User"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals:"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/Checklist.html","text":"Level 100: AWS Account Root User: Best Practice Checklist [ ] Root user has no access keys [ ] Root user is protected by a strong password with MFA stored securely [ ] Enable security questions in account [ ] Account credential report is regularly reviewed preferably automated [ ] AWS account email address to a secured, limited distribution list [ ] AWS account phone number for account to trusted contact [ ] Organizations with control policies for multiple accounts with stack sets","title":"Level 100: AWS Account & Root User: Best Practice Checklist"},{"location":"Security/100_AWS_Account_and_Root_User/Checklist.html#level-100-aws-account-root-user-best-practice-checklist","text":"[ ] Root user has no access keys [ ] Root user is protected by a strong password with MFA stored securely [ ] Enable security questions in account [ ] Account credential report is regularly reviewed preferably automated [ ] AWS account email address to a secured, limited distribution list [ ] AWS account phone number for account to trusted contact [ ] Organizations with control policies for multiple accounts with stack sets","title":"Level 100: AWS Account &amp; Root User: Best Practice Checklist"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html","text":"Level 100: AWS Account and Root User: Lab Guide 1. Account Settings Root User Security When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . 1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html 1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page . 1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update. 1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update. 1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone. 1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained. 1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy. 2. Tear down this lab Please note that the changes you made to the account and root user have no charges associated with them. References useful resources: AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: AWS Account and Root User: Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#level-100-aws-account-and-root-user-lab-guide","text":"","title":"Level 100: AWS Account and Root User: Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#1-account-settings-root-user-security","text":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"1. Account Settings &amp; Root User Security"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#11-generate-and-review-the-aws-account-credential-report","text":"Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html","title":"1.1 Generate and Review the AWS Account Credential Report"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#12-enable-a-virtual-mfa-device-for-your-aws-account-root-user","text":"You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .","title":"1.2 Enable a Virtual MFA Device for Your AWS Account Root User"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#13-configure-account-security-challenge-questions","text":"Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update.","title":"1.3 Configure Account Security Challenge Questions"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#14-configure-account-alternate-contacts","text":"Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update.","title":"1.4 Configure Account Alternate Contacts"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#15-remove-your-aws-account-root-user-access-keys","text":"You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone.","title":"1.5 Remove Your AWS Account Root User Access Keys"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#16-periodically-change-the-aws-account-root-user-password","text":"You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained.","title":"1.6 Periodically Change the AWS Account Root User Password"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#17-configure-a-strong-password-policy-for-your-users","text":"You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy.","title":"1.7 Configure a Strong Password Policy for Your Users"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#2-tear-down-this-lab","text":"Please note that the changes you made to the account and root user have no charges associated with them.","title":"2. Tear down this lab"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#references-useful-resources","text":"AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working","title":"References &amp; useful resources:"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html","text":"Level 100: Basic Identity and Access Management User, Group, Role Introduction This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting AWS credentials Fine-grained authorization Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Basic Identity and Access Management User, Group, Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#level-100-basic-identity-and-access-management-user-group-role","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals:"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html","text":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide 1. AWS Identity Access Management As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group. 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: 1. Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Users and then click Add user . 3. For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. 4. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. 5. Click Next: Permissions . 6. On the Set permissions for user page, click Add user to group . 7. Click Create group . 8. In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. 9. In the policy list, select the check box next to AdministratorAccess . Then click Create group . 10. Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. 11. Click Next: Tags . For this lab we will not add tags to the user. 12. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . 13. Configure MFA on your new administrator user by choosing Users from the navigation pane. 14. In the User Name list, click the name of the intended MFA user. 15. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. 16. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role. 1.2 Create Administrator IAM Role To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions 4. Tick AdministratorAccess from the list, and then click Next: Tags . 5. Click Next: Review 5. Enter a role name, e.g. 'Administrators' then click Create role . 6. Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. 6. The role is now created, with full administrative access and MFA enforced. 2. Assume Administrator Role from an IAM user We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console. 2.1 Use Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 3. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#level-100-basic-identity-and-access-management-user-group-role-lab-guide","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#1-aws-identity-access-management","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group.","title":"1. AWS Identity &amp; Access Management"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: 1. Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Users and then click Add user . 3. For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. 4. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. 5. Click Next: Permissions . 6. On the Set permissions for user page, click Add user to group . 7. Click Create group . 8. In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. 9. In the policy list, select the check box next to AdministratorAccess . Then click Create group . 10. Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. 11. Click Next: Tags . For this lab we will not add tags to the user. 12. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . 13. Configure MFA on your new administrator user by choosing Users from the navigation pane. 14. In the User Name list, click the name of the intended MFA user. 15. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. 16. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role.","title":"1.1 Create Administrator IAM User and Group"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#12-create-administrator-iam-role","text":"To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: 1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions 4. Tick AdministratorAccess from the list, and then click Next: Tags . 5. Click Next: Review 5. Enter a role name, e.g. 'Administrators' then click Create role . 6. Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. 6. The role is now created, with full administrative access and MFA enforced.","title":"1.2 Create Administrator IAM Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#2-assume-administrator-role-from-an-iam-user","text":"We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console.","title":"2. Assume Administrator Role from an IAM user"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#21-use-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Administrator Role in Web Console"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#3-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.","title":"3. Tear down this lab"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases","title":"References &amp; useful resources:"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html","text":"Level 100: CloudFront with S3 Bucket Origin Introduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting S3 bucket from direct public access Improving access time with caching Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#level-100-cloudfront-with-s3-bucket-origin","text":"","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#introduction","text":"This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#goals","text":"Protecting S3 bucket from direct public access Improving access time with caching","title":"Goals:"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites:"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html","text":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide 1. Create S3 bucket Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. From the console dashboard, choose Create bucket . 3. Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create . 2. Upload example index.html file Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list. 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . 2. From the console dashboard, click Create Distribution . 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In the Origin Domain Name field Select the S3 bucket you created previously. * In Restrict Bucket Access click the Yes radio then click Create a New Identity . * Click the Yes, Update Bucket Policy Button . * Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html * Click Create Distribution. * To return to the main CloudFront page click Distributions from the left navigation menu. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 4. Tear down this lab The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. Check the box next to the bucket you created previously, then click Empty from the menu. 3. Confirm the bucket you are emptying. 4. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. 5. Confirm the bucket you are deleting. References useful resources: Amazon S3 Developer Guide Amazon CloudFront Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#level-100-cloudfront-with-s3-bucket-origin-lab-guide","text":"","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#1-create-s3-bucket","text":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. From the console dashboard, choose Create bucket . 3. Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create .","title":"1. Create S3 bucket"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#2-upload-example-indexhtml-file","text":"Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list.","title":"2. Upload example index.html file"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . 2. From the console dashboard, click Create Distribution . 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In the Origin Domain Name field Select the S3 bucket you created previously. * In Restrict Bucket Access click the Yes radio then click Create a New Identity . * Click the Yes, Update Bucket Policy Button . * Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html * Click Create Distribution. * To return to the main CloudFront page click Distributions from the left navigation menu. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: 1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . 2. Check the box next to the bucket you created previously, then click Empty from the menu. 3. Confirm the bucket you are emptying. 4. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. 5. Confirm the bucket you are deleting.","title":"4. Tear down this lab"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#references-useful-resources","text":"Amazon S3 Developer Guide Amazon CloudFront Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Create_a_Data_Bunker/README.html","text":"Level 100: Create a Data Bunker Account Overview In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups. Prerequisites An AWS account that you have administrative access which is the root account for an AWS Organization. NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details. Detailed Instructions 1. Create a Security account from the master account Login to your master account Navigate to AWS Organizations and select Create Account . Include a cross account access role - we will modify this later to remove unnecessary access. Navigate to Settings and take a note of your Organization ID 2. Create the bucket for CloudTrail logs Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { Version : 2012-10-17 , Statement : [ { Sid : AWSCloudTrailAclCheck20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:GetBucketAcl , Resource : arn:aws:s3:::[bucket] }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/[organization id]/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days 3. Correct the IAM permissions so only cross account access is read-only Navigate to IAM and select Roles Select the OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy 4. Turn on CloudTrail from the root account Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Name the trail Organization Trail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and instead select the bucket created in the security account created previously Verification Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Create a Data Bunker Account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#level-100-create-a-data-bunker-account","text":"","title":"Level 100: Create a Data Bunker Account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#overview","text":"In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.","title":"Overview"},{"location":"Security/100_Create_a_Data_Bunker/README.html#prerequisites","text":"An AWS account that you have administrative access which is the root account for an AWS Organization. NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details.","title":"Prerequisites"},{"location":"Security/100_Create_a_Data_Bunker/README.html#detailed-instructions","text":"","title":"Detailed Instructions"},{"location":"Security/100_Create_a_Data_Bunker/README.html#1-create-a-security-account-from-the-master-account","text":"Login to your master account Navigate to AWS Organizations and select Create Account . Include a cross account access role - we will modify this later to remove unnecessary access. Navigate to Settings and take a note of your Organization ID","title":"1. Create a Security account from the master account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#2-create-the-bucket-for-cloudtrail-logs","text":"Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { Version : 2012-10-17 , Statement : [ { Sid : AWSCloudTrailAclCheck20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:GetBucketAcl , Resource : arn:aws:s3:::[bucket] }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/[organization id]/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days","title":"2. Create the bucket for CloudTrail logs"},{"location":"Security/100_Create_a_Data_Bunker/README.html#3-correct-the-iam-permissions-so-only-cross-account-access-is-read-only","text":"Navigate to IAM and select Roles Select the OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy","title":"3. Correct the IAM permissions so only cross account access is read-only"},{"location":"Security/100_Create_a_Data_Bunker/README.html#4-turn-on-cloudtrail-from-the-root-account","text":"Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Name the trail Organization Trail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and instead select the bucket created in the security account created previously","title":"4. Turn on CloudTrail from the root account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#verification","text":"Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail","title":"Verification"},{"location":"Security/100_Create_a_Data_Bunker/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/README.html","text":"Level 100: Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Goals: Enable AWS Security Hub Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Enable Security Hub"},{"location":"Security/100_Enable_Security_Hub/README.html#level-100-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Level 100: Enable Security Hub"},{"location":"Security/100_Enable_Security_Hub/README.html#goals","text":"Enable AWS Security Hub","title":"Goals:"},{"location":"Security/100_Enable_Security_Hub/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/100_Enable_Security_Hub/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Enable_Security_Hub/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html","text":"Level 100: Enable AWS Security Hub via AWS Console Authors Pierre Liddle, Principal Security Architect Table of Contents Getting Started 1. Getting Started The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub. 1.1 AWS Security Hub Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console 1.2 Enable AWS Security Hub In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub 1.3 Explore AWS Security Hub With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers. References useful resources: AWS Security Hub License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Enable AWS Security Hub via AWS Console"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#level-100-enable-aws-security-hub-via-aws-console","text":"","title":"Level 100: Enable AWS Security Hub via AWS Console"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#authors","text":"Pierre Liddle, Principal Security Architect","title":"Authors"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#table-of-contents","text":"Getting Started","title":"Table of Contents"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#1-getting-started","text":"The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub.","title":"1. Getting Started "},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#11-aws-security-hub","text":"Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console","title":"1.1 AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#12-enable-aws-security-hub","text":"In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub","title":"1.2 Enable AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#13-explore-aws-security-hub","text":"With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers.","title":"1.3 Explore AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#references-useful-resources","text":"AWS Security Hub","title":"References &amp; useful resources:"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html","text":"Level 200: Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Implement detective controls Automate security best practices Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#level-200-automated-deployment-of-detective-controls","text":"","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#goals","text":"Implement detective controls Automate security best practices","title":"Goals:"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html","text":"Level 200: Automated Deployment of Detective Controls: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Deployment Knowledge Check Tear Down 1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format. 2. Knowledge Check The security best practices followed in this lab are: * Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. * Implement new security services and features: New features such as Amazon GuardDuty have been adopted. * Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. * Implement managed services: Managed services are utilized to increase your visibility and control of your environment. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the DetectiveControls stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: 1. Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. 2. Select the CloudTrail bucket name you previously created without clicking the name. 3. Click Empty bucket and enter the bucket name in the confirmation box. 4. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. 5. With the bucket now empty, click Delete bucket. 6. Enter the bucket name in the confirmation box and click Confirm. 7. Repeat steps 2 to 6 for the Config bucket you created. References useful resources: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#level-200-automated-deployment-of-detective-controls-lab-guide","text":"","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#table-of-contents","text":"Deployment Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#1-aws-cloudformation-to-configure-aws-cloudtrail-aws-config-and-amazon-guardduty","text":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format.","title":"1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#2-knowledge-check","text":"The security best practices followed in this lab are: * Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. * Implement new security services and features: New features such as Amazon GuardDuty have been adopted. * Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. * Implement managed services: Managed services are utilized to increase your visibility and control of your environment.","title":"2. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the DetectiveControls stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: 1. Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. 2. Select the CloudTrail bucket name you previously created without clicking the name. 3. Click Empty bucket and enter the bucket name in the confirmation box. 4. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. 5. With the bucket now empty, click Delete bucket. 6. Enter the bucket name in the confirmation box and click Confirm. 7. Repeat steps 2 to 6 for the Config bucket you created.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#references-useful-resources","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"References &amp; useful resources:"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html","text":"Level 200: Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Goals: EC2 automated deployment Autoscaling and load balancing Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#goals","text":"EC2 automated deployment Autoscaling and load balancing","title":"Goals:"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC .","title":"Prerequisites:"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html","text":"Level 200: Automated Deployment of EC2 Web Application Authors Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected Table of Contents Overview Create Web Stack Knowledge Check Further Considerations Tear Down 1. Overview Overview of wordpress stack architecture: 2. Create Web Stack Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. 1. Choose the version of the CloudFormation template and download to your computer or by cloning this repository: * wordpress.yaml to create a WordPress site, including an RDS database. * staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. * ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). 10. In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. 3. Knowledge Check The security best practices followed in this lab are: * Grant access through roles or federation: A role is attached to the auto-scaled instances. * Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. * Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. * Implement new security services and features: New features including secrets manager have been adopted. * Limit exposure: Security groups restrict network traffic to a minimum. * Automate configuration management: CloudFormation is being used to deploy the application automatically. * Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. * Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. * Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. * Implement secure key management: AWS Key Management Service is used for key management of Aurora database. * Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error. 4. Further considerations: Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack. 5. Tear down this lab The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. 5. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/ References useful resources: AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#table-of-contents","text":"Overview Create Web Stack Knowledge Check Further Considerations Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#1-overview","text":"Overview of wordpress stack architecture:","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#2-create-web-stack","text":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. 1. Choose the version of the CloudFormation template and download to your computer or by cloning this repository: * wordpress.yaml to create a WordPress site, including an RDS database. * staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. * ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). 10. In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created.","title":"2. Create Web Stack "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: * Grant access through roles or federation: A role is attached to the auto-scaled instances. * Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. * Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. * Implement new security services and features: New features including secrets manager have been adopted. * Limit exposure: Security groups restrict network traffic to a minimum. * Automate configuration management: CloudFormation is being used to deploy the application automatically. * Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. * Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. * Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. * Implement secure key management: AWS Key Management Service is used for key management of Aurora database. * Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#4-further-considerations","text":"Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack.","title":"4. Further considerations: "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#5-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. 5. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/","title":"5. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources:"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html","text":"Level 200: Automated Deployment of IAM Groups and Roles Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Fine-grained authorization Automate security best practices Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#level-200-automated-deployment-of-iam-groups-and-roles","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#goals","text":"Fine-grained authorization Automate security best practices","title":"Goals:"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html","text":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide 1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account. 1.1 Create AWS CloudFormation Stack Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security! 2. Assume Roles from an IAM user We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user. 2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#level-200-automated-deployment-of-iam-groups-and-roles-lab-guide","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#1-aws-cloudformation-to-create-a-groups-policies-and-roles-with-mfa-enforced","text":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account.","title":"1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#11-create-aws-cloudformation-stack","text":"Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security!","title":"1.1 Create AWS CloudFormation Stack"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#2-assume-roles-from-an-iam-user","text":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.","title":"2. Assume Roles from an IAM user"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#21-use-restricted-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Restricted Administrator Role in Web Console"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#22-use-restricted-administrator-role-in-command-line-interface-cli","text":"Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html","title":"2.2 Use Restricted Administrator Role in Command Line Interface (CLI)"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button.","title":"3. Tear down this lab"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide","title":"References &amp; useful resources:"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html","text":"Level 200: Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Goals: VPC security features VPC layered subnet architecture Automated deployments Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#goals","text":"VPC security features VPC layered subnet architecture Automated deployments","title":"Goals:"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates.","title":"Prerequisites:"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html","text":"Level 200: Automated Deployment of VPC Authors Ben Potter, Security Lead, Well-Architected Table of Contents Overview Create VPC Stack Knowledge Check Tear Down 1. Overview 2. Create VPC Stack This step will create the VPC and all components using the example CloudFormation template. 1. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you). 3. Knowledge Check The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. * Implement new security services and features: New features including secrets manager have been adopted. * Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. * Automate configuration management: CloudFormation is being used to deploy the networking constructs. * Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. 4. Tear down this lab The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-VPC stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. Delete the CloudWatch Logs: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . 2. Click Logs in the left navigation. 3. Click the radio button on the left of the WebApp1-VPCFlowLog . 4. Click the Actions Button then click Delete Log Group . 5. Verify the log group name then click Yes, Delete . References useful resources: AWS CloudFormation User Guide Amazon VPC User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#table-of-contents","text":"Overview Create VPC Stack Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#1-overview","text":"","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#2-create-vpc-stack","text":"This step will create the VPC and all components using the example CloudFormation template. 1. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. 2. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. 3. Click Create Stack. 4. Click Upload a template file and then click Choose file . 5. Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . 5. Enter the following details: * Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". 6. At the bottom of the page click Next . 7. In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . 8. Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . 9. After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you).","title":"2. Create VPC Stack "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. * Implement new security services and features: New features including secrets manager have been adopted. * Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. * Automate configuration management: CloudFormation is being used to deploy the networking constructs. * Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . 2. Click the radio button on the left of the WebApp1-VPC stack. 3. Click the Actions button then click Delete stack . 4. Confirm the stack and then click Delete button. Delete the CloudWatch Logs: 1. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . 2. Click Logs in the left navigation. 3. Click the radio button on the left of the WebApp1-VPCFlowLog . 4. Click the Actions Button then click Delete Log Group . 5. Verify the log group name then click Yes, Delete .","title":"4. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon VPC User Guide","title":"References &amp; useful resources:"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html","text":"Level 200: Automated Deployment of Web Application Firewall Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Web Application Firewall"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#level-200-automated-deployment-of-web-application-firewall","text":"","title":"Level 200: Automated Deployment of Web Application Firewall"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites:"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html","text":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure WAF Configure CloudFront for WAF Tear Down 1. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 2. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources: Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#level-200-automated-deployment-of-web-application-firewall-lab-guide","text":"","title":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#table-of-contents","text":"Configure WAF Configure CloudFront for WAF Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#1-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"1. Configure AWS WAF "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#2-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"2. Configure Amazon CloudFront "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html","text":"Level 200: EC2 Web Infrastructure Protection Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#level-200-ec2-web-infrastructure-protection","text":"","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints .","title":"Prerequisites:"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html","text":"Level 200: EC2 Web Infrastructure Protection: Lab Guide 1. Launch Instance For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console. 1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Create AWS WAF Rules 2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use! 3. Create Application Load Balancer with WAF integration Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test. 3.1 Create Application Load Balancer Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successfull message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing. 3.2 Configure Application Load Balancer with WAF Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser. 4. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: 1. Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the left console instance menu, select Instances. 3. Select the instance you created to terminate. 4. From the Actions button (or right click) select Instance State Terminate. 5. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Load Balancers from the Load Balancing section. 3. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. 4. Confirm by clicking Yes, Delete. 5. From the console dashboard, choose Target Groups from the Load Balancing section. 3. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: 1. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the lab-waf-regional stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources: Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#level-200-ec2-web-infrastructure-protection-lab-guide","text":"","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.","title":"1. Launch Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#11-launch-single-linux-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . 7.3 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1.1 Launch Single Linux Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#2-create-aws-waf-rules","text":"","title":"2. Create AWS WAF Rules"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#21-aws-cloudformation-to-create-aws-waf-acl-for-application-load-balancer","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use!","title":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#3-create-application-load-balancer-with-waf-integration","text":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.","title":"3. Create Application Load Balancer with WAF integration"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#31-create-application-load-balancer","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successfull message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing.","title":"3.1 Create Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#32-configure-application-load-balancer-with-waf","text":"Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser.","title":"3.2 Configure Application Load Balancer with WAF"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: 1. Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the left console instance menu, select Instances. 3. Select the instance you created to terminate. 4. From the Actions button (or right click) select Instance State Terminate. 5. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Load Balancers from the Load Balancing section. 3. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. 4. Confirm by clicking Yes, Delete. 5. From the console dashboard, choose Target Groups from the Load Balancing section. 3. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: 1. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the lab-waf-regional stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"4. Tear down this lab"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/README.html","text":"Level 200: CloudFront for Web Application Introduction This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#level-200-cloudfront-for-web-application","text":"","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront.","title":"Prerequisites:"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html","text":"Level 200: CloudFront for Web Application: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure CloudFront - EC2 or Load Balancer Tear Down 1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 2. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. References useful resources: Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#level-200-cloudfront-for-web-application-lab-guide","text":"","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#table-of-contents","text":"Configure CloudFront - EC2 or Load Balancer Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#1-configure-amazon-cloudfront-for-ec2-or-elastic-load-balancer","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#2-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.","title":"2. Tear down this lab "},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#references-useful-resources","text":"Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html","text":"Level 200: CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#level-200-cloudfront-with-waf-protection","text":"","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals:"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites:"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html","text":"Level 200: CloudFront with WAF Protection: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Launch Instance Configure WAF Configure CloudFront Tear Down 1. Launch Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and click Next: Add tags . 7. Click Next: Configure Security Group . 7.1 Accept default opton Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources: Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#level-200-cloudfront-with-waf-protection-lab-guide","text":"","title":"Level 200: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#table-of-contents","text":"Launch Instance Configure WAF Configure CloudFront Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: 1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. 2. From the console dashboard, choose Launch Instance. 3. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. 4. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. 5. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and click Next: Add tags . 7. Click Next: Configure Security Group . 7.1 Accept default opton Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. 8. On the Review Instance Launch page, check the details, and then click Launch. 9. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. Important This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1. Launch Instance "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#2-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"2. Configure AWS WAF "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, choose Create Distribution. 3. Click Get Started in the Web section. 4. Specify the following settings for the distribution: * In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. * In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. * Click Create Distrubution. * For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. 5. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. 6. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: 1. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. 2. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. 3. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources:"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: IAM permission boundaries IAM policy conditions Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#goals","text":"IAM permission boundaries IAM policy conditions","title":"Goals:"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab. 1. Create IAM policies 1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. 1. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] } 1.3 Create developer IAM console access policy This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. 1. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] } 2. Create and Test Developer Role 2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. 5. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . 6. For this lab we will not use IAM tags, click Next: Review . 7. Enter the name of developer-restricted-iam for the Role name and click Create role . 8. Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test! 2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the developer role with the granted permissions, stay logged in using the role for the next section. 3. Create and Test User Role 3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . 1. Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . 4. In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. 5. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. 6. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . 7. For this lab we will not use IAM tags, click Next: Review . 8. Enter the Role name of app1-user-region-restricted-services for the role and click Create role . 9. The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps. 3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. 1. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. 2. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. 5. Select a different color to before, otherwise it will overwrite that profile in your browser. 6. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. 7. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! 8. Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. 9. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. 10. Congratulations! You have now learnt about IAM permission boundaries and have one working! 4. Knowledge Check The security best practices followed in this lab are: * Manage credentials and authentication Use of MFA for access to provide additional access control. * Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. 1. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read References useful resources: Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab.","title":"Table of Contents"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#1-create-iam-policies","text":"","title":"1. Create IAM policies "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#11-create-policy-for-permission-boundary","text":"This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy for permission boundary"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#12-create-developer-iam-restricted-policy","text":"This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. 1. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] }","title":"1.2 Create developer IAM restricted policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#13-create-developer-iam-console-access-policy","text":"This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. 1. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] }","title":"1.3 Create developer IAM console access policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#2-create-and-test-developer-role","text":"","title":"2. Create and Test Developer Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#21-create-developer-role","text":"Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. 5. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . 6. For this lab we will not use IAM tags, click Next: Review . 7. Enter the name of developer-restricted-iam for the Role name and click Create role . 8. Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test!","title":"2.1 Create Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#22-test-developer-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the developer role with the granted permissions, stay logged in using the role for the next section.","title":"2.2. Test Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#3-create-and-test-user-role","text":"","title":"3. Create and Test User Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#31-create-user-role","text":"While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . 1. Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . 4. In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. 5. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. 6. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . 7. For this lab we will not use IAM tags, click Next: Review . 8. Enter the Role name of app1-user-region-restricted-services for the role and click Create role . 9. The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps.","title":"3.1 Create User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#32-test-user-role","text":"Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. 1. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. 2. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. 3. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. 5. Select a different color to before, otherwise it will overwrite that profile in your browser. 6. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. 7. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! 8. Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. 9. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. 10. Congratulations! You have now learnt about IAM permission boundaries and have one working!","title":"3.2 Test User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: * Manage credentials and authentication Use of MFA for access to provide additional access control. * Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them. 1. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#references-useful-resources","text":"Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources:"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html","text":"Level 300: IAM Tag Based Access Control for EC2 Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: IAM least privilege IAM policy conditions Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#goals","text":"IAM least privilege IAM policy conditions","title":"Goals:"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html","text":"Level 300: IAM Tag Based Access Control for EC2 Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource. 1. Create IAM managed policies The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California). 1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:Describe* , ec2:Get* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. 1. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . 1. Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. 1. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. 1. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 2. Create Role Create a role for EC2 administrators, and attach the managed policies previously created. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . 5. For this lab we will not use IAM tags, click Next: Review . 6. Enter the name of ec2-admin-team-alpha for the Role name and click Create role . 8. Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test! 3. Test Role 3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 3.2 Launch Instance With Without Tags Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet. 3.3 Modify Tags On Instances Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed. 3.4 Manage Instances Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2! 4. Knowledge Check The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the policies and roles have no charges associated with them. 1. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances References useful resources: AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource.","title":"Table of Contents"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#1-create-iam-managed-policies","text":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).","title":"1. Create IAM managed policies "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#11-create-policy-named-ec2-list-read","text":"This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. 2. In the navigation pane, click Policies and then click Create policy . 3. On the Create policy page click the JSON tab. 4. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:Describe* , ec2:Get* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy named ec2-list-read"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#12-create-policy-named-ec2-create-tags","text":"This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. 1. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] }","title":"1.2 Create policy named ec2-create-tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#13-create-policy-named-ec2-create-tags-existing","text":"This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . 1. Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] }","title":"1.3 Create policy named ec2-create-tags-existing"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#14-create-policy-named-ec2-run-instances","text":"This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. 1. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.4 Create policy named ec2-run-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#15-create-policy-named-ec2-manage-instances","text":"This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. 1. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.5 Create policy named ec2-manage-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#2-create-role","text":"Create a role for EC2 administrators, and attach the managed policies previously created. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . 2. In the navigation pane, click Roles and then click Create role . 3. Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. 4. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . 5. For this lab we will not use IAM tags, click Next: Review . 6. Enter the name of ec2-admin-team-alpha for the Role name and click Create role . 8. Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. 9. The role is now created, ready to test!","title":"2. Create Role "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#3-test-role","text":"","title":"3. Test Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#31-assume-ec2-admin-team-alpha-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . 2. In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. 3. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. 4. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. 5. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. **Tip** The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.","title":"3.1 Assume ec2-admin-team-alpha Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#32-launch-instance-with-without-tags","text":"Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet.","title":"3.2 Launch Instance With &amp; Without Tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#33-modify-tags-on-instances","text":"Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed.","title":"3.3 Modify Tags On Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#34-manage-instances","text":"Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2!","title":"3.4 Manage Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the policies and roles have no charges associated with them. 1. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . 2. For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources:"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html","text":"Level 300: Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals: Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#introduction","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#goals","text":"Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response","title":"Goals:"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.","title":"Prerequisites:"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html","text":"Level 300: Incident Response with AWS Console and CLI Authors Ben Potter, Security Lead, Well-Architected Table of Contents Getting Started Identity Access Management Amazon VPC Knowledge Check 1. Getting Started 1.1 Install the AWS CLI Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced. 1.2 Amazon CloudWatch Logs Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1 2. Identity Access Management 2.1 Investigate AWS CloudTrail As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab. 2.1.1 AWS Console The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. 1. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. 2. From the left menu, choose Insights under Logs . 3. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. 4. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details. filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent 2.1.2 AWS CLI Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users. aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details. aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' 2.2 Block access in AWS IAM Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions. 2.3 List AWS IAM roles/users/groups If you need to confirm the name of a role, user or group you can list: 2.3.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field. 2.3.2 AWS CLI aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName' 2.4 Attach inline deny policy Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. 2.4.1 AWS Console Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy . 2.4.2 AWS CLI Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' 2.5 Delete inline deny policy To delete the policy you just attached and restore the original permissions the entity had: 2.5.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete 2.5.2 AWS CLI Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll 3. Amazon VPC A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled. 3.1 Investigate Amazon VPC Flow Logs 3.1.1 AWS Management Console The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. 1. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. 2. From the left menu, choose Insights under Logs . 3. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. 4. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc 4. Knowledge Check The security best practices followed in this lab are: * Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. * Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. * Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. * Implement managed services: Managed services are utilized to increase your visibility and control of your environment. * Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations. References useful resources: AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#table-of-contents","text":"Getting Started Identity Access Management Amazon VPC Knowledge Check","title":"Table of Contents"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#1-getting-started","text":"","title":"1. Getting Started "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#11-install-the-aws-cli","text":"Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.","title":"1.1 Install the AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#12-amazon-cloudwatch-logs","text":"Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1","title":"1.2 Amazon CloudWatch Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#2-identity-access-management","text":"","title":"2. Identity &amp; Access Management "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#21-investigate-aws-cloudtrail","text":"As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab.","title":"2.1 Investigate AWS CloudTrail"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#211-aws-console","text":"The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. 1. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. 2. From the left menu, choose Insights under Logs . 3. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. 4. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details. filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent","title":"2.1.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#212-aws-cli","text":"Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users. aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details. aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'","title":"2.1.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#22-block-access-in-aws-iam","text":"Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions.","title":"2.2 Block access in AWS IAM"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#23-list-aws-iam-rolesusersgroups","text":"If you need to confirm the name of a role, user or group you can list:","title":"2.3 List AWS IAM roles/users/groups"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#231-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field.","title":"2.3.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#232-aws-cli","text":"aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName'","title":"2.3.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#24-attach-inline-deny-policy","text":"Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations.","title":"2.4 Attach inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#241-aws-console","text":"Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy .","title":"2.4.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#242-aws-cli","text":"Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }'","title":"2.4.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#25-delete-inline-deny-policy","text":"To delete the policy you just attached and restore the original permissions the entity had:","title":"2.5 Delete inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#251-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete","title":"2.5.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#252-aws-cli","text":"Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll","title":"2.5.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#3-amazon-vpc","text":"A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled.","title":"3. Amazon VPC "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#31-investigate-amazon-vpc-flow-logs","text":"","title":"3.1 Investigate Amazon VPC Flow Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#311-aws-management-console","text":"The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. 1. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. 2. From the left menu, choose Insights under Logs . 3. From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. 4. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc","title":"3.1.1 AWS Management Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: * Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. * Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. * Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. * Implement managed services: Managed services are utilized to increase your visibility and control of your environment. * Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations.","title":"4. Knowledge Check "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#references-useful-resources","text":"AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax","title":"References &amp; useful resources:"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html","text":"Quest: Loft - Introduction to Security About this Guide This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Step 1 - New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Step 2 - Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Walkthrough Basic Identity and Access Management User, Group, Role Step 3 - CloudFront with WAF Protection This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying. Walkthrough CloudFront with WAF Protection Step 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail. Walkthrough Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#quest-loft-introduction-to-security","text":"","title":"Quest: Loft - Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#about-this-guide","text":"This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-1-new-aws-account-setup-and-securing-root-user","text":"","title":"Step 1 - New AWS Account Setup and Securing Root User"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-2-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Step 2 - Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_1","text":"Basic Identity and Access Management User, Group, Role","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-3-cloudfront-with-waf-protection","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying.","title":"Step 3 -  CloudFront with WAF Protection"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_2","text":"CloudFront with WAF Protection","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail.","title":"Step 4 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_3","text":"Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html","text":"Quest: One Day to Better Security About this Guide This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place. Step 1 - Multi-Account Strategy Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organisations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organisations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: * Organisations root account \u2013 used only for identity and billing * Shared services \u2013 for common tools such as deployment tooling * Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure Walkthrough Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organisations in the root account Invite any existing accounts For each AWS account required Create a new account in organisations . Make note of the organisations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account Step 2 - Identity Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions. Walkthrough Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1. Step 3 - Data Bunker Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organisation. Walkthrough Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail Step 4 - Enable organizations policies AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization. Walkthrough (repeat for each policy below) Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root Policy to prevent users disabling CloudTrail { Version : 2012-10-17 , Statement : [ { Effect : Deny , Action : cloudtrail:StopLogging , Resource : * } ] } (Optional) Disable unused regions This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ * ], Resource : [ * ], Condition : { ForAnyValue:StringNotEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Step 5 - Disable public access to data in S3 Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data. Walkthrough For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to shit to the root, apply this specifically to the accounts where you have blocked public access. Policy to block public { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ s3:PutBucketPublicAccessBlock , s3:PutAccountPublicAccessBlock ], Resource : * } ] } Step 6 - Monitoring and Alerting Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. Walkthrough Enable Security Hub Enable Amazon GuardDuty and implement basic detective controls Additional Resources and next steps Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Quick Steps to Security Success"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#quest-one-day-to-better-security","text":"","title":"Quest: One Day to Better Security"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#about-this-guide","text":"This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place.","title":"About this Guide"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-1-multi-account-strategy","text":"Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organisations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organisations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: * Organisations root account \u2013 used only for identity and billing * Shared services \u2013 for common tools such as deployment tooling * Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure","title":"Step 1 - Multi-Account Strategy"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough","text":"Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organisations in the root account Invite any existing accounts For each AWS account required Create a new account in organisations . Make note of the organisations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-2-identity","text":"Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions.","title":"Step 2 - Identity"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_1","text":"Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-3-data-bunker","text":"Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organisation.","title":"Step 3 - Data Bunker"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_2","text":"Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-4-enable-organizations-policies","text":"AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization.","title":"Step 4 - Enable organizations policies"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough-repeat-for-each-policy-below","text":"Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root","title":"Walkthrough (repeat for each policy below)"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-prevent-users-disabling-cloudtrail","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Deny , Action : cloudtrail:StopLogging , Resource : * } ] }","title":"Policy to prevent users disabling CloudTrail"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#optional-disable-unused-regions","text":"This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ * ], Resource : [ * ], Condition : { ForAnyValue:StringNotEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"(Optional) Disable unused regions"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-5-disable-public-access-to-data-in-s3","text":"Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data.","title":"Step 5 - Disable public access to data in S3"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_3","text":"For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to shit to the root, apply this specifically to the accounts where you have blocked public access.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-block-public","text":"{ Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ s3:PutBucketPublicAccessBlock , s3:PutAccountPublicAccessBlock ], Resource : * } ] }","title":"Policy to block public"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-6-monitoring-and-alerting","text":"Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs.","title":"Step 6 - Monitoring and Alerting"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_4","text":"Enable Security Hub Enable Amazon GuardDuty and implement basic detective controls","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#additional-resources-and-next-steps","text":"Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture","title":"Additional Resources and next steps"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html","text":"Quest: AWS Security Best Practices Day Authors Ben Potter, Security Lead, Well-Architected About this Guide This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1: Identity Access Management: For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed: Introductory Lab 1.1: AWS Account and Root User This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Lab 1.2 Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role Advanced Lab 1.3 - IAM Permission Boundaries Delegating Role Creation: This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1.4 - IAM Tag Based Access Control for EC2: This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of Detective Controls: This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 3 - Enable Security Hub: AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 4 - Automated Deployment of VPC: This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 5 - Automated Deployment of EC2 Web Application: This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 6 - Automated Deployment of Web Application Firewall: This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Automated Deployment of Web Application Firewall Lab 7 - CloudFront for Web Application: This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. CloudFront for Web Application Lab 8 - Incident Response with AWS Console and CLI: This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#quest-aws-security-best-practices-day","text":"","title":"Quest: AWS Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-1-identity-access-management","text":"For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed:","title":"Lab 1: Identity &amp; Access Management:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#introductory","text":"","title":"Introductory"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-11-aws-account-and-root-user","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Lab 1.1: AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-12-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Lab 1.2 Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#advanced","text":"","title":"Advanced"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-13-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1.3 - IAM Permission Boundaries Delegating Role Creation:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-14-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1.4 - IAM Tag Based Access Control for EC2:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-2-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 2 - Automated Deployment of Detective Controls:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-3-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 3 - Enable Security Hub:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-4-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 4 - Automated Deployment of VPC:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-5-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 5 - Automated Deployment of EC2 Web Application:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-6-automated-deployment-of-web-application-firewall","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods.","title":"Lab 6 - Automated Deployment of Web Application Firewall:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-web-application-firewall","text":"","title":"Automated Deployment of Web Application Firewall"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-7-cloudfront-for-web-application","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront.","title":"Lab 7 - CloudFront for Web Application:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#cloudfront-for-web-application","text":"","title":"CloudFront for Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-8-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 8 - Incident Response with AWS Console and CLI:"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html","text":"Quest: AWS Security Best Practices Workshop Authors Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect About this Guide This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Identity Access Management: For Lab 1 choose one of labs to run based on your interest or experience: Lab 1a - IAM Permission Boundaries Delegating Role Creation: This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1b - IAM Tag Based Access Control for EC2: This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of VPC: This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 3 - Automated Deployment of EC2 Web Application: This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 4 - Automated Deployment of Detective Controls: This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 5 - Enable Security Hub: AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 6 - Incident Response with AWS Console and CLI: This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#quest-aws-security-best-practices-workshop","text":"","title":"Quest: AWS Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1-identity-access-management","text":"For Lab 1 choose one of labs to run based on your interest or experience:","title":"Lab 1 -  Identity &amp; Access Management:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1a-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1a - IAM Permission Boundaries Delegating Role Creation:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1b-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1b - IAM Tag Based Access Control for EC2:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-2-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 2 - Automated Deployment of VPC:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-3-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 3 - Automated Deployment of EC2 Web Application:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 4 - Automated Deployment of Detective Controls:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-5-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 5 - Enable Security Hub:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-6-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 6 - Incident Response with AWS Console and CLI:"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Identity_Access_Management/README.html","text":"Quest: Identity Access Management Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architetced area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . New AWS Account Setup and Securing Root User: Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Basic Identity and Access Management User, Group, Role: Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role IAM Permission Boundaries Delegating Role Creation: Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2: Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Identity & Access Management"},{"location":"Security/Quest_Identity_Access_Management/README.html#quest-identity-access-management","text":"","title":"Quest: Identity &amp; Access Management"},{"location":"Security/Quest_Identity_Access_Management/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Identity_Access_Management/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architetced area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Identity_Access_Management/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"Security/Quest_Identity_Access_Management/README.html#new-aws-account-setup-and-securing-root-user","text":"","title":"New AWS Account Setup and Securing Root User:"},{"location":"Security/Quest_Identity_Access_Management/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Walkthrough"},{"location":"Security/Quest_Identity_Access_Management/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_Identity_Access_Management/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_Identity_Access_Management/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role:"},{"location":"Security/Quest_Identity_Access_Management/README.html#walkthrough_1","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Walkthrough"},{"location":"Security/Quest_Identity_Access_Management/README.html#basic-identity-and-access-management-user-group-role_1","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Identity_Access_Management/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation:"},{"location":"Security/Quest_Identity_Access_Management/README.html#walkthrough_2","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Walkthrough"},{"location":"Security/Quest_Identity_Access_Management/README.html#iam-permission-boundaries-delegating-role-creation_1","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Identity_Access_Management/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2:"},{"location":"Security/Quest_Identity_Access_Management/README.html#walkthrough_3","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Walkthrough"},{"location":"Security/Quest_Identity_Access_Management/README.html#iam-tag-based-access-control-for-ec2_1","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Identity_Access_Management/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Well-ArchitectedTool/README.html","text":"AWS Well-Architected Tool Labs http://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation . Labs: Level 100: Walkthrough of the AWS Well-Architected Tool License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Well-ArchitectedTool/README.html#aws-well-architected-tool-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Tool Labs"},{"location":"Well-ArchitectedTool/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation .","title":"Introduction"},{"location":"Well-ArchitectedTool/README.html#labs","text":"Level 100: Walkthrough of the AWS Well-Architected Tool","title":"Labs:"},{"location":"Well-ArchitectedTool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html","text":"Level 100: Walkthrough of the Well-Architected Tool http://wellarchitectedlabs.com Introduction The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework Goals: Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool. Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#level-100-walkthrough-of-the-well-architected-tool","text":"http://wellarchitectedlabs.com","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#introduction","text":"The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#goals","text":"Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.","title":"Goals:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy).","title":"Prequisites:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html","text":"Level 100: Walkthrough of the Well-Architected Tool Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Table of Contents Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down 1. Navigating to the console The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool : 2. Creating a workload Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button: 3. Performing a review From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit: 4. Saving a milestone From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it: 5. Viewing and downloading the report From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it. 6. Tear down this lab In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog: References useful resources: License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#level-100-walkthrough-of-the-well-architected-tool","text":"","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS","title":"Authors"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#table-of-contents","text":"Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down","title":"Table of Contents"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#1-navigating-to-the-console","text":"The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool :","title":"1. Navigating to the console "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#2-creating-a-workload","text":"Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button:","title":"2. Creating a workload "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#3-performing-a-review","text":"From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit:","title":"3. Performing a review "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#4-saving-a-milestone","text":"From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it:","title":"4. Saving a milestone "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#5-viewing-and-downloading-the-report","text":"From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it.","title":"5. Viewing and downloading the report "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#6-tear-down-this-lab","text":"In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog:","title":"6. Tear down this lab "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#references-useful-resources","text":"","title":"References &amp; useful resources:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"}]}